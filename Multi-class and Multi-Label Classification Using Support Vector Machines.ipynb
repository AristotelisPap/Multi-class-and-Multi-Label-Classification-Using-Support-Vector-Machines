{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 559 - Homework 6\n",
    "\n",
    "## Name: Aristotelis-Angelos Papadopoulos\n",
    "## USC ID: 3804-2945-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question a\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a dataframe with the dataset\n",
    "dataset = pd.read_csv('Frogs_MFCCs.csv', sep = \",\", header = 'infer')\n",
    "\n",
    "# Take 70% of the data for training and the rest for test\n",
    "train_set, test_set = train_test_split(dataset, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question b -> i**\n",
    "\n",
    "The Exact Match ratio and the Hamming loss methods for evaluating multi-label \n",
    "classification problems are well presented in the paper \"A Literature Survey on Algorithms for Multi-label\n",
    "Learning\" by Mohammad S. Sorower which can be found [here](https://www.researchgate.net/profile/Mohammad_Sorower/publication/266888594_A_Literature_Survey_on_Algorithms_for_Multi-label_Learning/links/58d1864392851cf4f8f4b72a/A-Literature-Survey-on-Algorithms-for-Multi-label-Learning.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question b -> ii\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# As the problem suggests, we will train a classifier for each label.\n",
    "# So, let us first extract the columns corresponding to these 3 labels.\n",
    "family_train = train_set['Family'] # Label 1\n",
    "genus_train = train_set['Genus'] # Label 2\n",
    "species_train = train_set['Species'] # Label 3\n",
    "features_train = train_set.iloc[:,0:22] \n",
    "\n",
    "family_test = test_set['Family'] # Label 1\n",
    "genus_test = test_set['Genus'] # Label 2\n",
    "species_test = test_set['Species'] # Label 3\n",
    "features_test = test_set.iloc[:,0:22] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 10000.0\n",
      "The best gamma is gamma= 2.3\n",
      "The CV error for those values is:  0.0057587362197410565\n",
      "For the label 'Family', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  100.0 %\n",
      "The cross-validation accuracy of the model is:  99.42412637802589 %\n",
      "The test accuracy for the label 'Family' is:  99.21259842519686 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> ii (Continue)\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Family\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the parameters C and gamma. Since, I did not have a problem\n",
    "# with the computational power, I created a grid of 400 points and picked\n",
    "# the values that resulted in the lowest CV error.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=10) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "gamma_range = np.linspace(0.1, 4, 40) # gamma ranges from 0.1 to 4 with linear increment\n",
    "\n",
    "\n",
    "for C1 in C_range:\n",
    "    for gamm in gamma_range: \n",
    "        # I will use 10-fold cross validation and I will shuffle\n",
    "        # the data before each split!\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "        skf.get_n_splits(features_train)\n",
    "        # Initialize a list in order to get the errors from 10-fold CV\n",
    "        list_10fold = []\n",
    "        for train_index, test_index in skf.split(features_train,family_train):\n",
    "            X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "            y_train, y_test = family_train.values[train_index], family_train.values[test_index]\n",
    "            \n",
    "            # Now, I will fit my SVM model in the k-1 folds!\n",
    "            classif = SVC(C=C1, kernel='rbf', gamma=gamm, decision_function_shape='ovr')\n",
    "            classif.fit(X_train, y_train)\n",
    "            list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "        # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "        ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(10,40) # Indexes are (C, gamma)\n",
    "# Take the indexes of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 10 different values of C\n",
    "# and the 40 different values of gamma!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The best gamma is gamma=\", gamma_range[ind[1]])\n",
    "print(\"The CV error for those values is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best values \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = SVC(C=C_range[ind[0]], kernel='rbf', gamma=gamma_range[ind[1]], decision_function_shape='ovr')\n",
    "classif.fit(features_train, family_train)\n",
    "\n",
    "print(\"For the label 'Family', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, family_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Family' is: \", classif.score(features_test, family_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 10000.0\n",
      "The best gamma is gamma= 2.1999999999999997\n",
      "The CV error for those values is:  0.008136772350812272\n",
      "For the label 'Genus', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  100.0 %\n",
      "The cross-validation accuracy of the model is:  99.18632276491877 %\n",
      "The test accuracy for the label 'Genus' is:  98.98100972672533 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> ii (Continue)\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Genus\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the parameters C and gamma. Since, I did not have a problem\n",
    "# with the computational power, I created a grid of 400 points and picked\n",
    "# the values that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=10) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "gamma_range = np.linspace(0.1, 4, 40) # gamma ranges from 0.1 to 4 with linear increment\n",
    "\n",
    "\n",
    "for C1 in C_range:\n",
    "    for gamm in gamma_range: \n",
    "        # I will use 10-fold cross validation and I will shuffle\n",
    "        # the data before each split!\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "        skf.get_n_splits(features_train)\n",
    "        # Initialize a list in order to get the errors from 10-fold CV\n",
    "        list_10fold = []\n",
    "        for train_index, test_index in skf.split(features_train,genus_train):\n",
    "            X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "            y_train, y_test = genus_train.values[train_index], genus_train.values[test_index]\n",
    "            \n",
    "            # Now, I will fit my SVM model in the k-1 folds!\n",
    "            classif = SVC(C=C1, kernel='rbf', gamma=gamm, decision_function_shape='ovr')\n",
    "            classif.fit(X_train, y_train)\n",
    "            list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "        # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "        ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(10,40) # Indexes are (C, gamma)\n",
    "# Take the indexes of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 10 different values of C\n",
    "# and the 40 different values of gamma!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The best gamma is gamma=\", gamma_range[ind[1]])\n",
    "print(\"The CV error for those values is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best values \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = SVC(C=C_range[ind[0]], kernel='rbf', gamma=gamma_range[ind[1]], decision_function_shape='ovr')\n",
    "classif.fit(features_train, genus_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Genus', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, genus_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Genus' is: \", classif.score(features_test, genus_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 100.0\n",
      "The best gamma is gamma= 2.3\n",
      "The CV error for those values is:  0.008339586823110168\n",
      "For the label 'Species', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  100.0 %\n",
      "The cross-validation accuracy of the model is:  99.16604131768898 %\n",
      "The test accuracy for the label 'Species' is:  98.98100972672533 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> ii (Continue)\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Species\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the parameters C and gamma. Since, I did not have a problem\n",
    "# with the computational power, I created a grid of 400 points and picked\n",
    "# the values that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=10) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "gamma_range = np.linspace(0.1, 4, 40) # gamma ranges from 0.1 to 4 with linear increment\n",
    "\n",
    "\n",
    "for C1 in C_range:\n",
    "    for gamm in gamma_range: \n",
    "        # I will use 10-fold cross validation and I will shuffle\n",
    "        # the data before each split!\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "        skf.get_n_splits(features_train)\n",
    "        # Initialize a list in order to get the errors from 10-fold CV\n",
    "        list_10fold = []\n",
    "        for train_index, test_index in skf.split(features_train,species_train):\n",
    "            X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "            y_train, y_test = species_train.values[train_index], species_train.values[test_index]\n",
    "            \n",
    "            # Now, I will fit my SVM model in the k-1 folds!\n",
    "            classif = SVC(C=C1, kernel='rbf', gamma=gamm, decision_function_shape='ovr')\n",
    "            classif.fit(X_train, y_train)\n",
    "            list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "        # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "        ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(10,40) # Indexes are (C, gamma)\n",
    "# Take the indexes of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 10 different values of C\n",
    "# and the 40 different values of gamma!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The best gamma is gamma=\", gamma_range[ind[1]])\n",
    "print(\"The CV error for those values is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best values \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = SVC(C=C_range[ind[0]], kernel='rbf', gamma=gamma_range[ind[1]], decision_function_shape='ovr')\n",
    "classif.fit(features_train, species_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Species', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, species_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Species' is: \", classif.score(features_test, species_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, using Binary Relevance with SVM with Gaussian Kernel, we have: \n",
      "\n",
      "The Exact Match score is: 0.9856415006947661\n",
      "The Hamming Loss is: 0.00941794040450826\n"
     ]
    }
   ],
   "source": [
    "# Question b -> ii (Continue)\n",
    "\n",
    "# Classifier for label 'Family'\n",
    "classif1 = SVC(C=10000, decision_function_shape='ovr', gamma=2.3, kernel='rbf')\n",
    "classif1.fit(features_train, family_train)\n",
    "\n",
    "# Classifier for label 'Genus'\n",
    "classif2 = SVC(C=10000, decision_function_shape='ovr', gamma=2.2, kernel='rbf')\n",
    "classif2.fit(features_train, genus_train)\n",
    "\n",
    "# Classifier for label 'Species'\n",
    "classif3 = SVC(C=100, decision_function_shape='ovr', gamma=2.3, kernel='rbf')\n",
    "classif3.fit(features_train, species_train)\n",
    "\n",
    "# Now, I will concatenate the 3 predicted labels into 1 numpy array\n",
    "y_pred = np.concatenate((np.expand_dims(classif1.predict(features_test), axis=1), \n",
    "                         np.expand_dims(classif2.predict(features_test), axis=1),\n",
    "                         np.expand_dims(classif3.predict(features_test), axis=1)), axis=1)\n",
    "\n",
    "# Now, I will convert the true labels into a numpy array\n",
    "y_true = np.concatenate((np.expand_dims(np.asarray(family_test), axis=1), \n",
    "                         np.expand_dims(np.asarray(genus_test), axis=1),\n",
    "                         np.expand_dims(np.asarray(species_test), axis=1)), axis=1)\n",
    "\n",
    "# Calculate the Exact Match score and the Hamming Loss\n",
    "EMscore = 0\n",
    "count_Ham = 0\n",
    "for i in range(0,y_true.shape[0]):\n",
    "    count_EM = 0\n",
    "    for j in range(0,y_true.shape[1]):\n",
    "        if y_true[i][j]==y_pred[i][j]:\n",
    "            count_EM += 1\n",
    "        else:\n",
    "            count_Ham += 1\n",
    "    if count_EM==3:\n",
    "        EMscore += 1\n",
    "EMscore = EMscore / y_true.shape[0]\n",
    "Hamming_Loss = count_Ham / (y_true.shape[0] * y_true.shape[1])\n",
    "            \n",
    "print(\"So, using Binary Relevance with SVM with Gaussian Kernel, we have:\",\"\\n\")\n",
    "print(\"The Exact Match score is:\", EMscore)\n",
    "print(\"The Hamming Loss is:\", Hamming_Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 2.848035868435802\n",
      "The CV error for this value is:  0.061545746729767446\n",
      "For the label 'Family', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  93.86417791898332 %\n",
      "The cross-validation accuracy of the model is:  93.84542532702326 %\n",
      "The test accuracy for the label 'Family' is:  93.42288096340899 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iii\n",
    "\n",
    "# In this question, we will apply the L1-penalized SVM algorithm.\n",
    "# Note that we will not normalize the attributes since they were\n",
    "# already normalized!\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Family\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the L1 regularization coefficient. \n",
    "# Since I did not have a problem with the computational power, I created a grid \n",
    "# of 100 points and picked the value that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=100) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "\n",
    "for C1 in C_range: \n",
    "    # I will use 10-fold cross validation and I will shuffle\n",
    "    # the data before each split!\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "    skf.get_n_splits(features_train)\n",
    "    # Initialize a list in order to get the errors from 10-fold CV\n",
    "    list_10fold = []\n",
    "    for train_index, test_index in skf.split(features_train,family_train):\n",
    "        X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "        y_train, y_test = family_train.values[train_index], family_train.values[test_index]\n",
    "            \n",
    "        # Now, I will fit my SVM model in the k-1 folds!\n",
    "        classif = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, C=C1, multi_class='ovr')\n",
    "        classif.fit(X_train, y_train)\n",
    "        list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "    # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "    ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(100,1) # Indexes are (C)\n",
    "# Take the index of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 100 different values of C!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The CV error for this value is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best value \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=C_range[ind[0]], multi_class='ovr')\n",
    "classif.fit(features_train, family_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Family', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, family_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Family' is: \", classif.score(features_test, family_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 18738.174228603868\n",
      "The CV error for this value is:  0.04605887310243938\n",
      "For the label 'Genus', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  95.59173947577443 %\n",
      "The cross-validation accuracy of the model is:  95.39411268975606 %\n",
      "The test accuracy for the label 'Genus' is:  95.83140342751274 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iii (Continue)\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Genus\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the L1 regularization coefficient. \n",
    "# Since I did not have a problem with the computational power, I created a grid \n",
    "# of 100 points and picked the value that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=100) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "\n",
    "for C1 in C_range: \n",
    "    # I will use 10-fold cross validation and I will shuffle\n",
    "    # the data before each split!\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "    skf.get_n_splits(features_train)\n",
    "    # Initialize a list in order to get the errors from 10-fold CV\n",
    "    list_10fold = []\n",
    "    for train_index, test_index in skf.split(features_train,genus_train):\n",
    "        X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "        y_train, y_test = genus_train.values[train_index], genus_train.values[test_index]\n",
    "            \n",
    "        # Now, I will fit my SVM model in the k-1 folds!\n",
    "        classif = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, C=C1, multi_class='ovr')\n",
    "        classif.fit(X_train, y_train)\n",
    "        list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "    # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "    ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(100,1) # Indexes are (C)\n",
    "# Take the index of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 100 different values of C!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The CV error for this value is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best value \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=C_range[ind[0]], multi_class='ovr')\n",
    "classif.fit(features_train, genus_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Genus', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, genus_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Genus' is: \", classif.score(features_test, genus_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 1519.9110829529332\n",
      "The CV error for this value is:  0.03931985181947569\n",
      "For the label 'Species', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  96.44559173947577 %\n",
      "The cross-validation accuracy of the model is:  96.06801481805243 %\n",
      "The test accuracy for the label 'Species' is:  96.34089856415007 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iii (Continue)\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Species\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the L1 regularization coefficient. \n",
    "# Since I did not have a problem with the computational power, I created a grid \n",
    "# of 100 points and picked the value that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=100) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "\n",
    "for C1 in C_range: \n",
    "    # I will use 10-fold cross validation and I will shuffle\n",
    "    # the data before each split!\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "    skf.get_n_splits(features_train)\n",
    "    # Initialize a list in order to get the errors from 10-fold CV\n",
    "    list_10fold = []\n",
    "    for train_index, test_index in skf.split(features_train,species_train):\n",
    "        X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "        y_train, y_test = species_train.values[train_index], species_train.values[test_index]\n",
    "            \n",
    "        # Now, I will fit my SVM model in the k-1 folds!\n",
    "        classif = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, C=C1, multi_class='ovr')\n",
    "        classif.fit(X_train, y_train)\n",
    "        list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "    # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "    ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(100,1) # Indexes are (C)\n",
    "# Take the index of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 100 different values of C!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The CV error for this value is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best value \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=C_range[ind[0]], multi_class='ovr')\n",
    "classif.fit(features_train, species_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Species', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, species_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Species' is: \", classif.score(features_test, species_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, using Binary Relevance with L1-penalized SVM, we have: \n",
      "\n",
      "The Exact Match score is: 0.9170912459471978\n",
      "The Hamming Loss is: 0.04801605681642736\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iii (Continue)\n",
    "\n",
    "# Classifier for label 'Family'\n",
    "classif4 = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=2.848, multi_class='ovr')\n",
    "classif4.fit(features_train, family_train)\n",
    "\n",
    "# Classifier for label 'Genus'\n",
    "classif5 = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=18738.2, multi_class='ovr')\n",
    "classif5.fit(features_train, genus_train)\n",
    "\n",
    "# Classifier for label 'Species'\n",
    "classif6 = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=1519.9, multi_class='ovr')\n",
    "classif6.fit(features_train, species_train)\n",
    "\n",
    "# Now, I will concatenate the 3 predicted labels into 1 numpy array\n",
    "y_pred2 = np.concatenate((np.expand_dims(classif4.predict(features_test), axis=1), \n",
    "                         np.expand_dims(classif5.predict(features_test), axis=1),\n",
    "                         np.expand_dims(classif6.predict(features_test), axis=1)), axis=1)\n",
    "\n",
    "# Now, I will convert the true labels into a numpy array\n",
    "y_true = np.concatenate((np.expand_dims(np.asarray(family_test), axis=1), \n",
    "                         np.expand_dims(np.asarray(genus_test), axis=1),\n",
    "                         np.expand_dims(np.asarray(species_test), axis=1)), axis=1)\n",
    "\n",
    "# Calculate the Exact Match score and the Hamming Loss\n",
    "EMscore = 0\n",
    "count_Ham = 0\n",
    "for i in range(0,y_true.shape[0]):\n",
    "    count_EM = 0\n",
    "    for j in range(0,y_true.shape[1]):\n",
    "        if y_true[i][j]==y_pred2[i][j]:\n",
    "            count_EM += 1\n",
    "        else:\n",
    "            count_Ham += 1\n",
    "    if count_EM==3:\n",
    "        EMscore += 1\n",
    "EMscore = EMscore / y_true.shape[0]\n",
    "Hamming_Loss = count_Ham / (y_true.shape[0] * y_true.shape[1])\n",
    "            \n",
    "print(\"So, using Binary Relevance with L1-penalized SVM, we have:\",\"\\n\")\n",
    "print(\"The Exact Match score is:\", EMscore)\n",
    "print(\"The Hamming Loss is:\", Hamming_Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 18738.174228603868\n",
      "The CV error for this value is:  0.07843023762711585\n",
      "For the label 'Family' using SMOTE, we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  93.88403494837172 %\n",
      "The cross-validation accuracy of the model is:  92.15697623728842 %\n",
      "The test accuracy for the label 'Family' is:  93.23761000463178 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iv\n",
    "\n",
    "# In this question, we are asked to use SMOTE in order to remedy\n",
    "# the class imbalance appearing in the dataset.\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# In this question, we will apply the L1-penalized SVM algorithm.\n",
    "# Note that we will not normalize the attributes since they were\n",
    "# already normalized!\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Family\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the L1 regularization coefficient. \n",
    "# Since I did not have a problem with the computational power, I created a grid \n",
    "# of 100 points and picked the value that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=100) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "\n",
    "for C1 in C_range: \n",
    "    # I will use 10-fold cross validation and I will shuffle\n",
    "    # the data before each split!\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "    skf.get_n_splits(features_train)\n",
    "    # Initialize a list in order to get the errors from 10-fold CV\n",
    "    list_10fold = []\n",
    "    for train_index, test_index in skf.split(features_train,family_train):\n",
    "        X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "        y_train, y_test = family_train.values[train_index], family_train.values[test_index]\n",
    "        \n",
    "        # Use SMOTE on the training data\n",
    "        sm = SMOTE(random_state=12, ratio = 'all')\n",
    "        X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "            \n",
    "        # Now, I will fit my SVM model in the k-1 folds!\n",
    "        classif = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, C=C1, multi_class='ovr')\n",
    "        classif.fit(X_train_res, y_train_res)\n",
    "        list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "    # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "    ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(100,1) # Indexes are (C)\n",
    "# Take the index of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 100 different values of C!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The CV error for this value is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best value \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=C_range[ind[0]], multi_class='ovr')\n",
    "classif.fit(features_train, family_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Family' using SMOTE, we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, family_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Family' is: \", classif.score(features_test, family_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 4.328761281083062\n",
      "The CV error for this value is:  0.08538843580758301\n",
      "For the label 'Genus' using SMOTE, we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  95.35345512311359 %\n",
      "The cross-validation accuracy of the model is:  91.46115641924169 %\n",
      "The test accuracy for the label 'Genus' is:  95.78508568781844 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iv (Continue)\n",
    "\n",
    "# In this question, we are asked to use SMOTE in order to remedy\n",
    "# the class imbalance appearing in the dataset.\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Genus\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the L1 regularization coefficient. \n",
    "# Since I did not have a problem with the computational power, I created a grid \n",
    "# of 100 points and picked the value that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=100) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "\n",
    "for C1 in C_range: \n",
    "    # I will use 10-fold cross validation and I will shuffle\n",
    "    # the data before each split!\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "    skf.get_n_splits(features_train)\n",
    "    # Initialize a list in order to get the errors from 10-fold CV\n",
    "    list_10fold = []\n",
    "    for train_index, test_index in skf.split(features_train,genus_train):\n",
    "        X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "        y_train, y_test = genus_train.values[train_index], genus_train.values[test_index]\n",
    "        \n",
    "        # Use SMOTE on the training data\n",
    "        sm = SMOTE(random_state=12, ratio = 'all')\n",
    "        X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "            \n",
    "        # Now, I will fit my SVM model in the k-1 folds!\n",
    "        classif = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, C=C1, multi_class='ovr')\n",
    "        classif.fit(X_train_res, y_train_res)\n",
    "        list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "    # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "    ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(100,1) # Indexes are (C)\n",
    "# Take the index of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 100 different values of C!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The CV error for this value is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best value \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=C_range[ind[0]], multi_class='ovr')\n",
    "classif.fit(features_train, genus_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Genus' using SMOTE, we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, genus_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Genus' is: \", classif.score(features_test, genus_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 6.5793322465756825\n",
      "The CV error for this value is:  0.04227783612861772\n",
      "For the label 'Species' using SMOTE, we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  96.36616362192217 %\n",
      "The cross-validation accuracy of the model is:  95.77221638713823 %\n",
      "The test accuracy for the label 'Species' is:  96.29458082445576 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iv (Continue)\n",
    "\n",
    "# In this question, we are asked to use SMOTE in order to remedy\n",
    "# the class imbalance appearing in the dataset.\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Species\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the L1 regularization coefficient. \n",
    "# Since I did not have a problem with the computational power, I created a grid \n",
    "# of 100 points and picked the value that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=100) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "\n",
    "for C1 in C_range: \n",
    "    # I will use 10-fold cross validation and I will shuffle\n",
    "    # the data before each split!\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "    skf.get_n_splits(features_train)\n",
    "    # Initialize a list in order to get the errors from 10-fold CV\n",
    "    list_10fold = []\n",
    "    for train_index, test_index in skf.split(features_train,species_train):\n",
    "        X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "        y_train, y_test = species_train.values[train_index], species_train.values[test_index]\n",
    "        \n",
    "        # Use SMOTE on the training data\n",
    "        sm = SMOTE(random_state=12, ratio = 'all')\n",
    "        X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "            \n",
    "        # Now, I will fit my SVM model in the k-1 folds!\n",
    "        classif = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, C=C1, multi_class='ovr')\n",
    "        classif.fit(X_train_res, y_train_res)\n",
    "        list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "    # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "    ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(100,1) # Indexes are (C)\n",
    "# Take the index of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 100 different values of C!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The CV error for this value is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best value \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=C_range[ind[0]], multi_class='ovr')\n",
    "classif.fit(features_train, species_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Species' using SMOTE, we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, species_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Species' is: \", classif.score(features_test, species_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, using Binary Relevance with L1-penalized SVM and the SMOTE technique on the training dataset, we have: \n",
      "\n",
      "The Exact Match score is: 0.8703103288559518\n",
      "The Hamming Loss is: 0.06855025474756832\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iv (Continue)\n",
    "\n",
    "# Use SMOTE\n",
    "X_train_res7, y_train_res7 = sm.fit_sample(features_train, family_train)\n",
    "\n",
    "# Classifier for label 'Family'\n",
    "classif7 = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=18738.2, multi_class='ovr')\n",
    "classif7.fit(X_train_res7, y_train_res7)\n",
    "\n",
    "# Use SMOTE\n",
    "X_train_res8, y_train_res8 = sm.fit_sample(features_train, genus_train)\n",
    "\n",
    "# Classifier for label 'Genus'\n",
    "classif8 = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=4.3, multi_class='ovr')\n",
    "classif8.fit(X_train_res8, y_train_res8)\n",
    "\n",
    "# Use SMOTE\n",
    "X_train_res9, y_train_res9 = sm.fit_sample(features_train, species_train)\n",
    "\n",
    "# Classifier for label 'Species'\n",
    "classif9 = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=6.6, multi_class='ovr')\n",
    "classif9.fit(X_train_res9, y_train_res9)\n",
    "\n",
    "# Now, I will concatenate the 3 predicted labels into 1 numpy array\n",
    "y_pred3 = np.concatenate((np.expand_dims(classif7.predict(features_test), axis=1), \n",
    "                         np.expand_dims(classif8.predict(features_test), axis=1),\n",
    "                         np.expand_dims(classif9.predict(features_test), axis=1)), axis=1)\n",
    "\n",
    "# Now, I will convert the true labels into a numpy array\n",
    "y_true = np.concatenate((np.expand_dims(np.asarray(family_test), axis=1), \n",
    "                         np.expand_dims(np.asarray(genus_test), axis=1),\n",
    "                         np.expand_dims(np.asarray(species_test), axis=1)), axis=1)\n",
    "\n",
    "# Calculate the Exact Match score and the Hamming Loss\n",
    "EMscore = 0\n",
    "count_Ham = 0\n",
    "for i in range(0,y_true.shape[0]):\n",
    "    count_EM = 0\n",
    "    for j in range(0,y_true.shape[1]):\n",
    "        if y_true[i][j]==y_pred3[i][j]:\n",
    "            count_EM += 1\n",
    "        else:\n",
    "            count_Ham += 1\n",
    "    if count_EM==3:\n",
    "        EMscore += 1\n",
    "EMscore = EMscore / y_true.shape[0]\n",
    "Hamming_Loss = count_Ham / (y_true.shape[0] * y_true.shape[1])\n",
    "            \n",
    "print(\"So, using Binary Relevance with L1-penalized SVM and the SMOTE technique on the training dataset, we have:\",\"\\n\")\n",
    "print(\"The Exact Match score is:\", EMscore)\n",
    "print(\"The Hamming Loss is:\", Hamming_Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
