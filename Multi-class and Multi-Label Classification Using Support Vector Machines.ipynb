{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 559 - Homework 6\n",
    "\n",
    "## Name: Aristotelis-Angelos Papadopoulos\n",
    "## USC ID: 3804-2945-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question a\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a dataframe with the dataset\n",
    "dataset = pd.read_csv('Frogs_MFCCs.csv', sep = \",\", header = 'infer')\n",
    "\n",
    "# Take 70% of the data for training and the rest for test\n",
    "train_set, test_set = train_test_split(dataset, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question b -> i**\n",
    "\n",
    "The Exact Match ratio and the Hamming loss methods for evaluating multi-label \n",
    "classification problems are well presented in the paper \"A Literature Survey on Algorithms for Multi-label\n",
    "Learning\" by Mohammad S. Sorower which can be found [here](https://www.researchgate.net/profile/Mohammad_Sorower/publication/266888594_A_Literature_Survey_on_Algorithms_for_Multi-label_Learning/links/58d1864392851cf4f8f4b72a/A-Literature-Survey-on-Algorithms-for-Multi-label-Learning.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question b -> ii\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# As the problem suggests, we will train a classifier for each label.\n",
    "# So, let us first extract the columns corresponding to these 3 labels.\n",
    "family_train = train_set['Family'] # Label 1\n",
    "genus_train = train_set['Genus'] # Label 2\n",
    "species_train = train_set['Species'] # Label 3\n",
    "features_train = train_set.iloc[:,0:22] \n",
    "\n",
    "family_test = test_set['Family'] # Label 1\n",
    "genus_test = test_set['Genus'] # Label 2\n",
    "species_test = test_set['Species'] # Label 3\n",
    "features_test = test_set.iloc[:,0:22] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 10000.0\n",
      "The best gamma is gamma= 2.3\n",
      "The CV error for those values is:  0.0057587362197410565\n",
      "For the label 'Family', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  100.0 %\n",
      "The cross-validation accuracy of the model is:  99.42412637802589 %\n",
      "The test accuracy for the label 'Family' is:  99.21259842519686 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> ii (Continue)\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Family\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the parameters C and gamma. Since, I did not have a problem\n",
    "# with the computational power, I created a grid of 400 points and picked\n",
    "# the values that resulted in the lowest CV error.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=10) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "gamma_range = np.linspace(0.1, 4, 40) # gamma ranges from 0.1 to 4 with linear increment\n",
    "\n",
    "\n",
    "for C1 in C_range:\n",
    "    for gamm in gamma_range: \n",
    "        # I will use 10-fold cross validation and I will shuffle\n",
    "        # the data before each split!\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "        skf.get_n_splits(features_train)\n",
    "        # Initialize a list in order to get the errors from 10-fold CV\n",
    "        list_10fold = []\n",
    "        for train_index, test_index in skf.split(features_train,family_train):\n",
    "            X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "            y_train, y_test = family_train.values[train_index], family_train.values[test_index]\n",
    "            \n",
    "            # Now, I will fit my SVM model in the k-1 folds!\n",
    "            classif = SVC(C=C1, kernel='rbf', gamma=gamm, decision_function_shape='ovr')\n",
    "            classif.fit(X_train, y_train)\n",
    "            list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "        # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "        ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(10,40) # Indexes are (C, gamma)\n",
    "# Take the indexes of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 10 different values of C\n",
    "# and the 40 different values of gamma!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The best gamma is gamma=\", gamma_range[ind[1]])\n",
    "print(\"The CV error for those values is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best values \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = SVC(C=C_range[ind[0]], kernel='rbf', gamma=gamma_range[ind[1]], decision_function_shape='ovr')\n",
    "classif.fit(features_train, family_train)\n",
    "\n",
    "print(\"For the label 'Family', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, family_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Family' is: \", classif.score(features_test, family_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 10000.0\n",
      "The best gamma is gamma= 2.1999999999999997\n",
      "The CV error for those values is:  0.008136772350812272\n",
      "For the label 'Genus', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  100.0 %\n",
      "The cross-validation accuracy of the model is:  99.18632276491877 %\n",
      "The test accuracy for the label 'Genus' is:  98.98100972672533 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> ii (Continue)\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Genus\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the parameters C and gamma. Since, I did not have a problem\n",
    "# with the computational power, I created a grid of 400 points and picked\n",
    "# the values that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=10) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "gamma_range = np.linspace(0.1, 4, 40) # gamma ranges from 0.1 to 4 with linear increment\n",
    "\n",
    "\n",
    "for C1 in C_range:\n",
    "    for gamm in gamma_range: \n",
    "        # I will use 10-fold cross validation and I will shuffle\n",
    "        # the data before each split!\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "        skf.get_n_splits(features_train)\n",
    "        # Initialize a list in order to get the errors from 10-fold CV\n",
    "        list_10fold = []\n",
    "        for train_index, test_index in skf.split(features_train,genus_train):\n",
    "            X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "            y_train, y_test = genus_train.values[train_index], genus_train.values[test_index]\n",
    "            \n",
    "            # Now, I will fit my SVM model in the k-1 folds!\n",
    "            classif = SVC(C=C1, kernel='rbf', gamma=gamm, decision_function_shape='ovr')\n",
    "            classif.fit(X_train, y_train)\n",
    "            list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "        # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "        ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(10,40) # Indexes are (C, gamma)\n",
    "# Take the indexes of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 10 different values of C\n",
    "# and the 40 different values of gamma!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The best gamma is gamma=\", gamma_range[ind[1]])\n",
    "print(\"The CV error for those values is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best values \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = SVC(C=C_range[ind[0]], kernel='rbf', gamma=gamma_range[ind[1]], decision_function_shape='ovr')\n",
    "classif.fit(features_train, genus_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Genus', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, genus_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Genus' is: \", classif.score(features_test, genus_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 100.0\n",
      "The best gamma is gamma= 2.3\n",
      "The CV error for those values is:  0.008339586823110168\n",
      "For the label 'Species', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  100.0 %\n",
      "The cross-validation accuracy of the model is:  99.16604131768898 %\n",
      "The test accuracy for the label 'Species' is:  98.98100972672533 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> ii (Continue)\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Species\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the parameters C and gamma. Since, I did not have a problem\n",
    "# with the computational power, I created a grid of 400 points and picked\n",
    "# the values that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=10) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "gamma_range = np.linspace(0.1, 4, 40) # gamma ranges from 0.1 to 4 with linear increment\n",
    "\n",
    "\n",
    "for C1 in C_range:\n",
    "    for gamm in gamma_range: \n",
    "        # I will use 10-fold cross validation and I will shuffle\n",
    "        # the data before each split!\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "        skf.get_n_splits(features_train)\n",
    "        # Initialize a list in order to get the errors from 10-fold CV\n",
    "        list_10fold = []\n",
    "        for train_index, test_index in skf.split(features_train,species_train):\n",
    "            X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "            y_train, y_test = species_train.values[train_index], species_train.values[test_index]\n",
    "            \n",
    "            # Now, I will fit my SVM model in the k-1 folds!\n",
    "            classif = SVC(C=C1, kernel='rbf', gamma=gamm, decision_function_shape='ovr')\n",
    "            classif.fit(X_train, y_train)\n",
    "            list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "        # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "        ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(10,40) # Indexes are (C, gamma)\n",
    "# Take the indexes of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 10 different values of C\n",
    "# and the 40 different values of gamma!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The best gamma is gamma=\", gamma_range[ind[1]])\n",
    "print(\"The CV error for those values is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best values \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = SVC(C=C_range[ind[0]], kernel='rbf', gamma=gamma_range[ind[1]], decision_function_shape='ovr')\n",
    "classif.fit(features_train, species_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Species', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, species_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Species' is: \", classif.score(features_test, species_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, using Binary Relevance with SVM with Gaussian Kernel, we have: \n",
      "\n",
      "The Exact Match score is: 0.9856415006947661\n",
      "The Hamming Loss is: 0.00941794040450826\n"
     ]
    }
   ],
   "source": [
    "# Question b -> ii (Continue)\n",
    "\n",
    "# Classifier for label 'Family'\n",
    "classif1 = SVC(C=10000, decision_function_shape='ovr', gamma=2.3, kernel='rbf')\n",
    "classif1.fit(features_train, family_train)\n",
    "\n",
    "# Classifier for label 'Genus'\n",
    "classif2 = SVC(C=10000, decision_function_shape='ovr', gamma=2.2, kernel='rbf')\n",
    "classif2.fit(features_train, genus_train)\n",
    "\n",
    "# Classifier for label 'Species'\n",
    "classif3 = SVC(C=100, decision_function_shape='ovr', gamma=2.3, kernel='rbf')\n",
    "classif3.fit(features_train, species_train)\n",
    "\n",
    "# Now, I will concatenate the 3 predicted labels into 1 numpy array\n",
    "y_pred = np.concatenate((np.expand_dims(classif1.predict(features_test), axis=1), \n",
    "                         np.expand_dims(classif2.predict(features_test), axis=1),\n",
    "                         np.expand_dims(classif3.predict(features_test), axis=1)), axis=1)\n",
    "\n",
    "# Now, I will convert the true labels into a numpy array\n",
    "y_true = np.concatenate((np.expand_dims(np.asarray(family_test), axis=1), \n",
    "                         np.expand_dims(np.asarray(genus_test), axis=1),\n",
    "                         np.expand_dims(np.asarray(species_test), axis=1)), axis=1)\n",
    "\n",
    "# Calculate the Exact Match score and the Hamming Loss\n",
    "EMscore = 0\n",
    "count_Ham = 0\n",
    "for i in range(0,y_true.shape[0]):\n",
    "    count_EM = 0\n",
    "    for j in range(0,y_true.shape[1]):\n",
    "        if y_true[i][j]==y_pred[i][j]:\n",
    "            count_EM += 1\n",
    "        else:\n",
    "            count_Ham += 1\n",
    "    if count_EM==3:\n",
    "        EMscore += 1\n",
    "EMscore = EMscore / y_true.shape[0]\n",
    "Hamming_Loss = count_Ham / (y_true.shape[0] * y_true.shape[1])\n",
    "            \n",
    "print(\"So, using Binary Relevance with SVM with Gaussian Kernel, we have:\",\"\\n\")\n",
    "print(\"The Exact Match score is:\", EMscore)\n",
    "print(\"The Hamming Loss is:\", Hamming_Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 2.848035868435802\n",
      "The CV error for this value is:  0.061545746729767446\n",
      "For the label 'Family', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  93.86417791898332 %\n",
      "The cross-validation accuracy of the model is:  93.84542532702326 %\n",
      "The test accuracy for the label 'Family' is:  93.42288096340899 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iii\n",
    "\n",
    "# In this question, we will apply the L1-penalized SVM algorithm.\n",
    "# Note that we will not normalize the attributes since they were\n",
    "# already normalized!\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Family\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the L1 regularization coefficient. \n",
    "# Since I did not have a problem with the computational power, I created a grid \n",
    "# of 100 points and picked the value that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=100) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "\n",
    "for C1 in C_range: \n",
    "    # I will use 10-fold cross validation and I will shuffle\n",
    "    # the data before each split!\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "    skf.get_n_splits(features_train)\n",
    "    # Initialize a list in order to get the errors from 10-fold CV\n",
    "    list_10fold = []\n",
    "    for train_index, test_index in skf.split(features_train,family_train):\n",
    "        X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "        y_train, y_test = family_train.values[train_index], family_train.values[test_index]\n",
    "            \n",
    "        # Now, I will fit my SVM model in the k-1 folds!\n",
    "        classif = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, C=C1, multi_class='ovr')\n",
    "        classif.fit(X_train, y_train)\n",
    "        list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "    # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "    ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(100,1) # Indexes are (C)\n",
    "# Take the index of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 100 different values of C!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The CV error for this value is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best value \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=C_range[ind[0]], multi_class='ovr')\n",
    "classif.fit(features_train, family_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Family', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, family_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Family' is: \", classif.score(features_test, family_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 18738.174228603868\n",
      "The CV error for this value is:  0.04605887310243938\n",
      "For the label 'Genus', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  95.59173947577443 %\n",
      "The cross-validation accuracy of the model is:  95.39411268975606 %\n",
      "The test accuracy for the label 'Genus' is:  95.83140342751274 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iii (Continue)\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Genus\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the L1 regularization coefficient. \n",
    "# Since I did not have a problem with the computational power, I created a grid \n",
    "# of 100 points and picked the value that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=100) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "\n",
    "for C1 in C_range: \n",
    "    # I will use 10-fold cross validation and I will shuffle\n",
    "    # the data before each split!\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "    skf.get_n_splits(features_train)\n",
    "    # Initialize a list in order to get the errors from 10-fold CV\n",
    "    list_10fold = []\n",
    "    for train_index, test_index in skf.split(features_train,genus_train):\n",
    "        X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "        y_train, y_test = genus_train.values[train_index], genus_train.values[test_index]\n",
    "            \n",
    "        # Now, I will fit my SVM model in the k-1 folds!\n",
    "        classif = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, C=C1, multi_class='ovr')\n",
    "        classif.fit(X_train, y_train)\n",
    "        list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "    # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "    ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(100,1) # Indexes are (C)\n",
    "# Take the index of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 100 different values of C!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The CV error for this value is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best value \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=C_range[ind[0]], multi_class='ovr')\n",
    "classif.fit(features_train, genus_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Genus', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, genus_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Genus' is: \", classif.score(features_test, genus_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 1519.9110829529332\n",
      "The CV error for this value is:  0.03931985181947569\n",
      "For the label 'Species', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  96.44559173947577 %\n",
      "The cross-validation accuracy of the model is:  96.06801481805243 %\n",
      "The test accuracy for the label 'Species' is:  96.34089856415007 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iii (Continue)\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Species\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the L1 regularization coefficient. \n",
    "# Since I did not have a problem with the computational power, I created a grid \n",
    "# of 100 points and picked the value that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=100) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "\n",
    "for C1 in C_range: \n",
    "    # I will use 10-fold cross validation and I will shuffle\n",
    "    # the data before each split!\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "    skf.get_n_splits(features_train)\n",
    "    # Initialize a list in order to get the errors from 10-fold CV\n",
    "    list_10fold = []\n",
    "    for train_index, test_index in skf.split(features_train,species_train):\n",
    "        X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "        y_train, y_test = species_train.values[train_index], species_train.values[test_index]\n",
    "            \n",
    "        # Now, I will fit my SVM model in the k-1 folds!\n",
    "        classif = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, C=C1, multi_class='ovr')\n",
    "        classif.fit(X_train, y_train)\n",
    "        list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "    # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "    ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(100,1) # Indexes are (C)\n",
    "# Take the index of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 100 different values of C!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The CV error for this value is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best value \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=C_range[ind[0]], multi_class='ovr')\n",
    "classif.fit(features_train, species_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Species', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, species_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Species' is: \", classif.score(features_test, species_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, using Binary Relevance with L1-penalized SVM, we have: \n",
      "\n",
      "The Exact Match score is: 0.920796665122742\n",
      "The Hamming Loss is: 0.04878801914466574\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iii (Continue)\n",
    "\n",
    "# Classifier for label 'Family'\n",
    "classif4 = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=2.848, multi_class='ovr')\n",
    "classif4.fit(features_train, family_train)\n",
    "\n",
    "# Classifier for label 'Genus'\n",
    "classif5 = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=18738.2, multi_class='ovr')\n",
    "classif5.fit(features_train, genus_train)\n",
    "\n",
    "# Classifier for label 'Species'\n",
    "classif6 = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=1519.9, multi_class='ovr')\n",
    "classif6.fit(features_train, species_train)\n",
    "\n",
    "# Now, I will concatenate the 3 predicted labels into 1 numpy array\n",
    "y_pred2 = np.concatenate((np.expand_dims(classif4.predict(features_test), axis=1), \n",
    "                         np.expand_dims(classif5.predict(features_test), axis=1),\n",
    "                         np.expand_dims(classif6.predict(features_test), axis=1)), axis=1)\n",
    "\n",
    "# Now, I will convert the true labels into a numpy array\n",
    "y_true = np.concatenate((np.expand_dims(np.asarray(family_test), axis=1), \n",
    "                         np.expand_dims(np.asarray(genus_test), axis=1),\n",
    "                         np.expand_dims(np.asarray(species_test), axis=1)), axis=1)\n",
    "\n",
    "# Calculate the Exact Match score and the Hamming Loss\n",
    "EMscore = 0\n",
    "count_Ham = 0\n",
    "for i in range(0,y_true.shape[0]):\n",
    "    count_EM = 0\n",
    "    for j in range(0,y_true.shape[1]):\n",
    "        if y_true[i][j]==y_pred2[i][j]:\n",
    "            count_EM += 1\n",
    "        else:\n",
    "            count_Ham += 1\n",
    "    if count_EM==3:\n",
    "        EMscore += 1\n",
    "EMscore = EMscore / y_true.shape[0]\n",
    "Hamming_Loss = count_Ham / (y_true.shape[0] * y_true.shape[1])\n",
    "            \n",
    "print(\"So, using Binary Relevance with L1-penalized SVM, we have:\",\"\\n\")\n",
    "print(\"The Exact Match score is:\", EMscore)\n",
    "print(\"The Hamming Loss is:\", Hamming_Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 18738.174228603868\n",
      "The CV error for this value is:  0.07843023762711585\n",
      "For the label 'Family' using SMOTE, we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  93.88403494837172 %\n",
      "The cross-validation accuracy of the model is:  92.15697623728842 %\n",
      "The test accuracy for the label 'Family' is:  93.23761000463178 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iv\n",
    "\n",
    "# In this question, we are asked to use SMOTE in order to remedy\n",
    "# the class imbalance appearing in the dataset.\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# In this question, we will apply the L1-penalized SVM algorithm.\n",
    "# Note that we will not normalize the attributes since they were\n",
    "# already normalized!\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Family\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the L1 regularization coefficient. \n",
    "# Since I did not have a problem with the computational power, I created a grid \n",
    "# of 100 points and picked the value that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=100) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "\n",
    "for C1 in C_range: \n",
    "    # I will use 10-fold cross validation and I will shuffle\n",
    "    # the data before each split!\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "    skf.get_n_splits(features_train)\n",
    "    # Initialize a list in order to get the errors from 10-fold CV\n",
    "    list_10fold = []\n",
    "    for train_index, test_index in skf.split(features_train,family_train):\n",
    "        X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "        y_train, y_test = family_train.values[train_index], family_train.values[test_index]\n",
    "        \n",
    "        # Use SMOTE on the training data\n",
    "        sm = SMOTE(random_state=12, ratio = 'all')\n",
    "        X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "            \n",
    "        # Now, I will fit my SVM model in the k-1 folds!\n",
    "        classif = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, C=C1, multi_class='ovr')\n",
    "        classif.fit(X_train_res, y_train_res)\n",
    "        list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "    # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "    ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(100,1) # Indexes are (C)\n",
    "# Take the index of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 100 different values of C!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The CV error for this value is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best value \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=C_range[ind[0]], multi_class='ovr')\n",
    "classif.fit(features_train, family_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Family' using SMOTE, we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, family_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Family' is: \", classif.score(features_test, family_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 4.328761281083062\n",
      "The CV error for this value is:  0.08538843580758301\n",
      "For the label 'Genus' using SMOTE, we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  95.35345512311359 %\n",
      "The cross-validation accuracy of the model is:  91.46115641924169 %\n",
      "The test accuracy for the label 'Genus' is:  95.78508568781844 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iv (Continue)\n",
    "\n",
    "# In this question, we are asked to use SMOTE in order to remedy\n",
    "# the class imbalance appearing in the dataset.\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Genus\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the L1 regularization coefficient. \n",
    "# Since I did not have a problem with the computational power, I created a grid \n",
    "# of 100 points and picked the value that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=100) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "\n",
    "for C1 in C_range: \n",
    "    # I will use 10-fold cross validation and I will shuffle\n",
    "    # the data before each split!\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "    skf.get_n_splits(features_train)\n",
    "    # Initialize a list in order to get the errors from 10-fold CV\n",
    "    list_10fold = []\n",
    "    for train_index, test_index in skf.split(features_train,genus_train):\n",
    "        X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "        y_train, y_test = genus_train.values[train_index], genus_train.values[test_index]\n",
    "        \n",
    "        # Use SMOTE on the training data\n",
    "        sm = SMOTE(random_state=12, ratio = 'all')\n",
    "        X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "            \n",
    "        # Now, I will fit my SVM model in the k-1 folds!\n",
    "        classif = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, C=C1, multi_class='ovr')\n",
    "        classif.fit(X_train_res, y_train_res)\n",
    "        list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "    # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "    ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(100,1) # Indexes are (C)\n",
    "# Take the index of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 100 different values of C!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The CV error for this value is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best value \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=C_range[ind[0]], multi_class='ovr')\n",
    "classif.fit(features_train, genus_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Genus' using SMOTE, we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, genus_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Genus' is: \", classif.score(features_test, genus_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 6.5793322465756825\n",
      "The CV error for this value is:  0.04227783612861772\n",
      "For the label 'Species' using SMOTE, we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  96.36616362192217 %\n",
      "The cross-validation accuracy of the model is:  95.77221638713823 %\n",
      "The test accuracy for the label 'Species' is:  96.29458082445576 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iv (Continue)\n",
    "\n",
    "# In this question, we are asked to use SMOTE in order to remedy\n",
    "# the class imbalance appearing in the dataset.\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Species\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the L1 regularization coefficient. \n",
    "# Since I did not have a problem with the computational power, I created a grid \n",
    "# of 100 points and picked the value that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=100) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "\n",
    "for C1 in C_range: \n",
    "    # I will use 10-fold cross validation and I will shuffle\n",
    "    # the data before each split!\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "    skf.get_n_splits(features_train)\n",
    "    # Initialize a list in order to get the errors from 10-fold CV\n",
    "    list_10fold = []\n",
    "    for train_index, test_index in skf.split(features_train,species_train):\n",
    "        X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "        y_train, y_test = species_train.values[train_index], species_train.values[test_index]\n",
    "        \n",
    "        # Use SMOTE on the training data\n",
    "        sm = SMOTE(random_state=12, ratio = 'all')\n",
    "        X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "            \n",
    "        # Now, I will fit my SVM model in the k-1 folds!\n",
    "        classif = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, C=C1, multi_class='ovr')\n",
    "        classif.fit(X_train_res, y_train_res)\n",
    "        list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "    # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "    ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(100,1) # Indexes are (C)\n",
    "# Take the index of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 100 different values of C!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The CV error for this value is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best value \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=C_range[ind[0]], multi_class='ovr')\n",
    "classif.fit(features_train, species_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Species' using SMOTE, we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, species_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Species' is: \", classif.score(features_test, species_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, using Binary Relevance with L1-penalized SVM and the SMOTE technique on the training dataset, we have: \n",
      "\n",
      "The Exact Match score is: 0.8703103288559518\n",
      "The Hamming Loss is: 0.06855025474756832\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iv (Continue)\n",
    "\n",
    "# Use SMOTE\n",
    "X_train_res7, y_train_res7 = sm.fit_sample(features_train, family_train)\n",
    "\n",
    "# Classifier for label 'Family'\n",
    "classif7 = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=18738.2, multi_class='ovr')\n",
    "classif7.fit(X_train_res7, y_train_res7)\n",
    "\n",
    "# Use SMOTE\n",
    "X_train_res8, y_train_res8 = sm.fit_sample(features_train, genus_train)\n",
    "\n",
    "# Classifier for label 'Genus'\n",
    "classif8 = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=4.3, multi_class='ovr')\n",
    "classif8.fit(X_train_res8, y_train_res8)\n",
    "\n",
    "# Use SMOTE\n",
    "X_train_res9, y_train_res9 = sm.fit_sample(features_train, species_train)\n",
    "\n",
    "# Classifier for label 'Species'\n",
    "classif9 = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=6.6, multi_class='ovr')\n",
    "classif9.fit(X_train_res9, y_train_res9)\n",
    "\n",
    "# Now, I will concatenate the 3 predicted labels into 1 numpy array\n",
    "y_pred3 = np.concatenate((np.expand_dims(classif7.predict(features_test), axis=1), \n",
    "                         np.expand_dims(classif8.predict(features_test), axis=1),\n",
    "                         np.expand_dims(classif9.predict(features_test), axis=1)), axis=1)\n",
    "\n",
    "# Now, I will convert the true labels into a numpy array\n",
    "y_true = np.concatenate((np.expand_dims(np.asarray(family_test), axis=1), \n",
    "                         np.expand_dims(np.asarray(genus_test), axis=1),\n",
    "                         np.expand_dims(np.asarray(species_test), axis=1)), axis=1)\n",
    "\n",
    "# Calculate the Exact Match score and the Hamming Loss\n",
    "EMscore = 0\n",
    "count_Ham = 0\n",
    "for i in range(0,y_true.shape[0]):\n",
    "    count_EM = 0\n",
    "    for j in range(0,y_true.shape[1]):\n",
    "        if y_true[i][j]==y_pred3[i][j]:\n",
    "            count_EM += 1\n",
    "        else:\n",
    "            count_Ham += 1\n",
    "    if count_EM==3:\n",
    "        EMscore += 1\n",
    "EMscore = EMscore / y_true.shape[0]\n",
    "Hamming_Loss = count_Ham / (y_true.shape[0] * y_true.shape[1])\n",
    "            \n",
    "print(\"So, using Binary Relevance with L1-penalized SVM and the SMOTE technique on the training dataset, we have:\",\"\\n\")\n",
    "print(\"The Exact Match score is:\", EMscore)\n",
    "print(\"The Hamming Loss is:\", Hamming_Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions for Binary Relevance\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "        **Classification Methods**\n",
    "        </td>\n",
    "        <td>\n",
    "        **Exact Match score**\n",
    "        </td>\n",
    "        <td>\n",
    "        **Hamming Loss**\n",
    "        </td>\n",
    "\n",
    "    </tr>\n",
    "        <td>\n",
    "        SVM with <br />\n",
    "        Gaussian kernel\n",
    "        </td>\n",
    "        <td>\n",
    "        0.985\n",
    "        </td>\n",
    "        <td>\n",
    "        0.0094\n",
    "        </td>\n",
    "    <tr>\n",
    "    </tr>\n",
    "        <td>\n",
    "        SVM with <br />\n",
    "        L1 regularization\n",
    "        </td>\n",
    "        <td>\n",
    "        0.92\n",
    "        </td>\n",
    "        <td>\n",
    "        0.048\n",
    "        </td>\n",
    "    <tr>\n",
    "        <td>\n",
    "        SVM with <br />\n",
    "        L1 regularization <br />\n",
    "        and SMOTE\n",
    "        </td>\n",
    "        <td>\n",
    "        0.87\n",
    "        </td>\n",
    "        <td>\n",
    "        0.0685\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be observed from the above table, the SVM with Gaussian kernel algorithm achieves the highest Exact Match score and the lowest Hamming Loss compared to the other two algorithms. This is happening since this algorithm creates nonlinear decision boundaries among the classes and the data are not fully linearly separable.\n",
    "\n",
    "However, it should be noted that the other two algorithms achieved good scores too even though the SMOTE approach required a lot of computational power in order to apply the 10-fold CV approach for 100 values of the regularization parameter C. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Chain method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 1000.0\n",
      "The best gamma is gamma= 2.5\n",
      "The CV error for those values is:  0.006553543766363435\n",
      "For the label 'Family', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  100.0 %\n",
      "The cross-validation accuracy of the model is:  99.34464562336366 %\n",
      "The test accuracy for the label 'Family' is:  99.39786938397405 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> v\n",
    "\n",
    "# At first, we have to train our algorithm for the 1st label, let's \n",
    "# say \"Family\" as we also did in the Binary Relevance case!\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Family\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the parameters C and gamma. Since, I did not have a problem\n",
    "# with the computational power, I created a grid of 400 points and picked\n",
    "# the values that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=10) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "gamma_range = np.linspace(0.1, 4, 40) # gamma ranges from 0.1 to 4 with linear increment\n",
    "\n",
    "\n",
    "for C1 in C_range:\n",
    "    for gamm in gamma_range: \n",
    "        # I will use 10-fold cross validation and I will shuffle\n",
    "        # the data before each split!\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "        skf.get_n_splits(features_train)\n",
    "        # Initialize a list in order to get the errors from 10-fold CV\n",
    "        list_10fold = []\n",
    "        for train_index, test_index in skf.split(features_train,family_train):\n",
    "            X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "            y_train, y_test = family_train.values[train_index], family_train.values[test_index]\n",
    "            \n",
    "            # Now, I will fit my SVM model in the k-1 folds!\n",
    "            classif = SVC(C=C1, kernel='rbf', gamma=gamm, decision_function_shape='ovr')\n",
    "            classif.fit(X_train, y_train)\n",
    "            list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "        # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "        ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(10,40) # Indexes are (C, gamma)\n",
    "# Take the indexes of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 10 different values of C\n",
    "# and the 40 different values of gamma!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The best gamma is gamma=\", gamma_range[ind[1]])\n",
    "print(\"The CV error for those values is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best values \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = SVC(C=C_range[ind[0]], kernel='rbf', gamma=gamma_range[ind[1]], decision_function_shape='ovr')\n",
    "classif.fit(features_train, family_train)\n",
    "\n",
    "print(\"For the label 'Family', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, family_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Family' is: \", classif.score(features_test, family_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 1000000.0\n",
      "The best gamma is gamma= 0.8999999999999999\n",
      "The CV error for those values is:  0.003373117840297446\n",
      "For the label 'Genus', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  100.0 %\n",
      "The cross-validation accuracy of the model is:  99.66268821597025 %\n"
     ]
    }
   ],
   "source": [
    "# Question b -> v (Continue)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Since the method \"fit\" cannot handle strings I will rename my classes.\n",
    "le.fit(family_train)\n",
    "family_train1 = le.transform(family_train)\n",
    "\n",
    "# Now, I have to add the label for Family as a \n",
    "# new column (feature) in my dataset and train the next classifier\n",
    "# for this augmented set of features!\n",
    "features_train1 = np.concatenate((features_train, \n",
    "                      np.expand_dims(family_train1, axis=1)), axis=1)\n",
    "#Convert it into a dataframe\n",
    "features_train_augment_1 = pd.DataFrame(data=features_train1,index=None,columns=None)\n",
    "\n",
    "# Now, I have to train the next classifier for the \n",
    "# augmented set of features!\n",
    "\n",
    "# We will train a SVM for the label \"Genus\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the parameters C and gamma. Since, I did not have a problem\n",
    "# with the computational power, I created a grid of 400 points and picked\n",
    "# the values that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=10) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "gamma_range = np.linspace(0.1, 4, 40) # gamma ranges from 0.1 to 4 with linear increment\n",
    "\n",
    "\n",
    "for C1 in C_range:\n",
    "    for gamm in gamma_range: \n",
    "        # I will use 10-fold cross validation and I will shuffle\n",
    "        # the data before each split!\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "        skf.get_n_splits(features_train_augment_1)\n",
    "        # Initialize a list in order to get the errors from 10-fold CV\n",
    "        list_10fold = []\n",
    "        for train_index, test_index in skf.split(features_train_augment_1,genus_train):\n",
    "            X_train, X_test = features_train_augment_1.values[train_index], features_train_augment_1.values[test_index]\n",
    "            y_train, y_test = genus_train.values[train_index], genus_train.values[test_index]\n",
    "            \n",
    "            # Now, I will fit my SVM model in the k-1 folds!\n",
    "            classif = SVC(C=C1, kernel='rbf', gamma=gamm, decision_function_shape='ovr')\n",
    "            classif.fit(X_train, y_train)\n",
    "            list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "        # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "        ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(10,40) # Indexes are (C, gamma)\n",
    "# Take the indexes of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 10 different values of C\n",
    "# and the 40 different values of gamma!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The best gamma is gamma=\", gamma_range[ind[1]])\n",
    "print(\"The CV error for those values is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best values \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = SVC(C=C_range[ind[0]], kernel='rbf', gamma=gamma_range[ind[1]], decision_function_shape='ovr')\n",
    "classif.fit(features_train_augment_1, genus_train)\n",
    "\n",
    "print(\"For the label 'Genus', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train_augment_1, genus_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 100.0\n",
      "The best gamma is gamma= 0.5\n",
      "The CV error for those values is:  0.0007877944257230762\n",
      "For the label 'Species', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  100.0 %\n",
      "The cross-validation accuracy of the model is:  99.92122055742769 %\n"
     ]
    }
   ],
   "source": [
    "# Question b -> v (Continue)\n",
    "\n",
    "# Since the method \"fit\" cannot handle strings I will rename my classes.\n",
    "le.fit(genus_train)\n",
    "genus_train1 = le.transform(genus_train)\n",
    "\n",
    "# Now, I have to add the label for Genus as a \n",
    "# new column (feature) in my already augmented dataset \n",
    "# and train the next classifier for this augmented set of features!\n",
    "features_train2 = np.concatenate((features_train1, \n",
    "                      np.expand_dims(genus_train1, axis=1)), axis=1)\n",
    "#Convert it into a dataframe\n",
    "features_train_augment_2 = pd.DataFrame(data=features_train2,index=None,columns=None)\n",
    "\n",
    "# Now, I have to train the next classifier for the \n",
    "# augmented set of features!\n",
    "\n",
    "# We will train a SVM for the label \"Species\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the parameters C and gamma. Since, I did not have a problem\n",
    "# with the computational power, I created a grid of 400 points and picked\n",
    "# the values that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=10) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "gamma_range = np.linspace(0.1, 4, 40) # gamma ranges from 0.1 to 4 with linear increment\n",
    "\n",
    "\n",
    "for C1 in C_range:\n",
    "    for gamm in gamma_range: \n",
    "        # I will use 10-fold cross validation and I will shuffle\n",
    "        # the data before each split!\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "        skf.get_n_splits(features_train_augment_2)\n",
    "        # Initialize a list in order to get the errors from 10-fold CV\n",
    "        list_10fold = []\n",
    "        for train_index, test_index in skf.split(features_train_augment_2,species_train):\n",
    "            X_train, X_test = features_train_augment_2.values[train_index], features_train_augment_2.values[test_index]\n",
    "            y_train, y_test = species_train.values[train_index], species_train.values[test_index]\n",
    "            \n",
    "            # Now, I will fit my SVM model in the k-1 folds!\n",
    "            classif = SVC(C=C1, kernel='rbf', gamma=gamm, decision_function_shape='ovr')\n",
    "            classif.fit(X_train, y_train)\n",
    "            list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "        # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "        ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(10,40) # Indexes are (C, gamma)\n",
    "# Take the indexes of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 10 different values of C\n",
    "# and the 40 different values of gamma!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The best gamma is gamma=\", gamma_range[ind[1]])\n",
    "print(\"The CV error for those values is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best values \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = SVC(C=C_range[ind[0]], kernel='rbf', gamma=gamma_range[ind[1]], decision_function_shape='ovr')\n",
    "classif.fit(features_train_augment_2, species_train)\n",
    "\n",
    "print(\"For the label 'Species', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train_augment_2, species_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, using Classifier Chain method with SVM with Gaussian Kernel, we have: \n",
      "\n",
      "The Exact Match score is: 0.9911996294580825\n",
      "The Hamming Loss is: 0.00725644588544079\n"
     ]
    }
   ],
   "source": [
    "# Question b -> v (Continue)\n",
    "\n",
    "# Classifier for label 'Family'\n",
    "classif10 = SVC(C=1000, decision_function_shape='ovr', gamma=2.5, kernel='rbf')\n",
    "classif10.fit(features_train, family_train)\n",
    "y1_pred = classif10.predict(features_test)\n",
    "\n",
    "# Now, I have to augment my features test dataset\n",
    "# with the predicted \"Family\" label from the first classifier\n",
    "# and let my second classifier predict the label \"Genus\" by\n",
    "# providing him the augmented set of features.\n",
    "\n",
    "# Since I changed the name before, I have to do it also for the test set.\n",
    "le.fit(y1_pred)\n",
    "y1_pred = le.transform(y1_pred)\n",
    "        \n",
    "features_test1 = np.concatenate((features_test, np.expand_dims(y1_pred, axis=1)), axis=1)\n",
    "#Convert it into a dataframe\n",
    "features_test_augment_1 = pd.DataFrame(data=features_test1,index=None,columns=None)\n",
    "        \n",
    "\n",
    "# Classifier for label 'Genus'\n",
    "classif11 = SVC(C=1000000, decision_function_shape='ovr', gamma=0.9, kernel='rbf')\n",
    "classif11.fit(features_train_augment_1, genus_train)\n",
    "y2_pred = classif11.predict(features_test_augment_1)\n",
    "                                       \n",
    "# Now, I have to augment my already augmented features test dataset\n",
    "# with the predicted \"Genus\" label from the second classifier\n",
    "# and let my third classifier predict the label \"Species\" by\n",
    "# providing him the augmented set of features.\n",
    "\n",
    "# Since I changed the name before, I have to do it also for the test set.\n",
    "le.fit(y2_pred)\n",
    "y2_pred = le.transform(y2_pred)\n",
    "        \n",
    "features_test2 = np.concatenate((features_test1, np.expand_dims(y2_pred, axis=1)), axis=1)\n",
    "#Convert it into a dataframe\n",
    "features_test_augment_2 = pd.DataFrame(data=features_test2,index=None,columns=None)\n",
    "\n",
    "# Classifier for label 'Species'\n",
    "classif12 = SVC(C=100, decision_function_shape='ovr', gamma=0.5, kernel='rbf')\n",
    "classif12.fit(features_train_augment_2, species_train)\n",
    "\n",
    "# Now, I will concatenate the 3 predicted labels into 1 numpy array\n",
    "y_pred4 = np.concatenate((np.expand_dims(classif10.predict(features_test), axis=1), \n",
    "                         np.expand_dims(classif11.predict(features_test_augment_1), axis=1),\n",
    "                         np.expand_dims(classif12.predict(features_test_augment_2), axis=1)), axis=1)\n",
    "\n",
    "# Now, I will convert the true labels into a numpy array\n",
    "y_true = np.concatenate((np.expand_dims(np.asarray(family_test), axis=1), \n",
    "                         np.expand_dims(np.asarray(genus_test), axis=1),\n",
    "                         np.expand_dims(np.asarray(species_test), axis=1)), axis=1)\n",
    "\n",
    "# Calculate the Exact Match score and the Hamming Loss\n",
    "EMscore = 0\n",
    "count_Ham = 0\n",
    "for i in range(0,y_true.shape[0]):\n",
    "    count_EM = 0\n",
    "    for j in range(0,y_true.shape[1]):\n",
    "        if y_true[i][j]==y_pred4[i][j]:\n",
    "            count_EM += 1\n",
    "        else:\n",
    "            count_Ham += 1\n",
    "    if count_EM==3:\n",
    "        EMscore += 1\n",
    "EMscore = EMscore / y_true.shape[0]\n",
    "Hamming_Loss = count_Ham / (y_true.shape[0] * y_true.shape[1])\n",
    "            \n",
    "print(\"So, using Classifier Chain method with SVM with Gaussian Kernel, we have:\",\"\\n\")\n",
    "print(\"The Exact Match score is:\", EMscore)\n",
    "print(\"The Hamming Loss is:\", Hamming_Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***: I implemented the Classifier Chain method only for the SVM with Gaussian kernel algorithm since this was the algorithm that had the best scores in the previous question! Similarily, I could have done it for the other algorithms too (L1 penalized SVC and L1 penalized SVC with SMOTE) but it needs a lot of computation effort to cross validate for all the parameters of the classifiers and I did not have enough time for simulations. However, the implementation for the SVM with Gaussian kernel is more than enough to show how the Classifier Chain method works compared to the Binary Relevance method. \n",
    "\n",
    "Comparing the results of the algorithm between Binary Relevance and Classifier Chain methods, we see that **the Classifier Chain method produced a higher Exact Match score and a lower Hamming Loss than the Binary Relevance method**. This behavior was expected since the Classifier Chain method takes into account the correlation between the labels while the Binary Relevance is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Futher Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrices for SVM with Gaussian kernel (Binary Relevance method)\n",
      "\n",
      "The confusion matrix for the label 'Family' is:\n",
      "[[ 676    3    0    0]\n",
      " [   9 1301    0    0]\n",
      " [   0    0  151    0]\n",
      " [   1    0    0   18]]\n",
      "\n",
      "\n",
      "The confusion matrix for the label 'Genus' is:\n",
      "[[1219    0    0    0    0    0    0]\n",
      " [   0  151    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0]\n",
      " [   0    0    0   83    0    0    0]\n",
      " [   0    0    0    0   35    0    0]\n",
      " [   0    0    0    0    0   18    0]\n",
      " [   0    0    0    0    0    0   52]]\n",
      "\n",
      "\n",
      "The confusion matrix for the label 'Species' is:\n",
      "[[ 207    0    0    1    1    1    0    0    0    0]\n",
      " [   0 1012    0    1    0    2    0    0    0    0]\n",
      " [   0    0  151    0    0    0    0    0    0    0]\n",
      " [   1    1    0   83    0    0    0    0    0    0]\n",
      " [   0    0    0    0  131    0    0    0    0    0]\n",
      " [   0    1    0    0    2  369    0    1    0    0]\n",
      " [   0    0    0    0    1    1   83    0    0    0]\n",
      " [   0    0    0    0    1    1    0   35    0    0]\n",
      " [   0    0    0    0    1    0    0    0   18    0]\n",
      " [   0    0    0    1    0    0    0    0    0   52]]\n"
     ]
    }
   ],
   "source": [
    "# Question b -> vi (Confusion Matrices)\n",
    "\n",
    "# We will first calculate the Confusion matrices for the problem.\n",
    "# In order to do this, we actually have to calculate 3 confusion\n",
    "# matrices (1 for each label)!\n",
    "\n",
    "# NOTE: The confusion matrices will be calculated ONLY for TEST data!!!\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Confusion Matrices for SVM with Gaussian kernel (Binary Relevance method)\\n\")\n",
    "# Confusion matrix for label \"Family\"\n",
    "print(\"The confusion matrix for the label 'Family' is:\")\n",
    "print(metrics.confusion_matrix(y_true[:,0], y_pred[:,0], \n",
    "                               labels=[\"Hylidae\", \"Leptodactylidae\",\n",
    "                                      \"Dendrobatidae\", \"Bufonidae\"]))\n",
    "\n",
    "# Confusion matrix for label \"Genus\"\n",
    "print(\"\\n\")\n",
    "print(\"The confusion matrix for the label 'Genus' is:\")\n",
    "print(metrics.confusion_matrix(y_true[:,1], y_pred[:,1], \n",
    "                               labels=[\"Adenomera\", \"Ameerega\", \"Dendropsophus\"\n",
    "                                      \"Hypsiboas\", \"Leptodactylus\", \"Osteocephalus\",\n",
    "                                      \"Rhinella\", \"Scinax\"]))\n",
    "\n",
    "# Confusion matrix for label \"Species\"\n",
    "print(\"\\n\")\n",
    "print(\"The confusion matrix for the label 'Species' is:\")\n",
    "print(metrics.confusion_matrix(y_true[:,2], y_pred[:,2], \n",
    "                               labels=[\"AdenomeraAndre\", \"AdenomeraHylaedactylus\", \n",
    "                                       \"Ameeregatrivittata\", \"HylaMinuta\",\n",
    "                                      \"HypsiboasCinerascens\", \"HypsiboasCordobae\",\n",
    "                                      \"LeptodactylusFuscus\", \"OsteocephalusOophagus\",\n",
    "                                      \"Rhinellagranulosa\", \"ScinaxRuber\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrices for Linear SVC with L1 regularization (Binary Relevance method)\n",
      "\n",
      "The confusion matrix for the label 'Family' is:\n",
      "[[ 631   47    1    0]\n",
      " [  28 1274    8    0]\n",
      " [  10    8  133    0]\n",
      " [  18    1    0    0]]\n",
      "\n",
      "\n",
      "The confusion matrix for the label 'Genus' is:\n",
      "[[1209    7    0    0    0    0    1]\n",
      " [   5  139    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0]\n",
      " [   1    0    0   73    1    1    0]\n",
      " [   1    0    0    0   18    0    0]\n",
      " [   1    0    0    1    0   14    0]\n",
      " [   0    0    0    0    0    0   52]]\n",
      "\n",
      "\n",
      "The confusion matrix for the label 'Species' is:\n",
      "[[ 198    0    7    0    0    4    0    0    0    1]\n",
      " [   0 1011    1    1    1    1    0    0    0    0]\n",
      " [   4    0  139    3    0    5    0    0    0    0]\n",
      " [  11    5    3   63    1    2    0    0    0    0]\n",
      " [   2    0    0    0  128    1    0    0    0    0]\n",
      " [   0    4    0    0    3  364    1    0    0    1]\n",
      " [   1    0    0    1    1    4   77    0    1    0]\n",
      " [   0    0    0    0    5   13    0   19    0    0]\n",
      " [   0    0    0    0    1    0    0    0   18    0]\n",
      " [   0    0    0    1    0    0    0    0    0   52]]\n"
     ]
    }
   ],
   "source": [
    "# Question b -> vi (Confusion Matrices)\n",
    "\n",
    "# NOTE: The confusion matrices will be calculated ONLY for TEST data!!!\n",
    "\n",
    "print(\"Confusion Matrices for Linear SVC with L1 regularization (Binary Relevance method)\\n\")\n",
    "# Confusion matrix for label \"Family\"\n",
    "print(\"The confusion matrix for the label 'Family' is:\")\n",
    "print(metrics.confusion_matrix(y_true[:,0], y_pred2[:,0], \n",
    "                               labels=[\"Hylidae\", \"Leptodactylidae\",\n",
    "                                      \"Dendrobatidae\", \"Bufonidae\"]))\n",
    "\n",
    "# Confusion matrix for label \"Genus\"\n",
    "print(\"\\n\")\n",
    "print(\"The confusion matrix for the label 'Genus' is:\")\n",
    "print(metrics.confusion_matrix(y_true[:,1], y_pred2[:,1], \n",
    "                               labels=[\"Adenomera\", \"Ameerega\", \"Dendropsophus\"\n",
    "                                      \"Hypsiboas\", \"Leptodactylus\", \"Osteocephalus\",\n",
    "                                      \"Rhinella\", \"Scinax\"]))\n",
    "\n",
    "# Confusion matrix for label \"Species\"\n",
    "print(\"\\n\")\n",
    "print(\"The confusion matrix for the label 'Species' is:\")\n",
    "print(metrics.confusion_matrix(y_true[:,2], y_pred2[:,2], \n",
    "                               labels=[\"AdenomeraAndre\", \"AdenomeraHylaedactylus\", \n",
    "                                       \"Ameeregatrivittata\", \"HylaMinuta\",\n",
    "                                      \"HypsiboasCinerascens\", \"HypsiboasCordobae\",\n",
    "                                      \"LeptodactylusFuscus\", \"OsteocephalusOophagus\",\n",
    "                                      \"Rhinellagranulosa\", \"ScinaxRuber\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrices for Linear SVC with L1 regularization and SMOTE (Binary Relevance method)\n",
      "\n",
      "The confusion matrix for the label 'Family' is:\n",
      "[[195 394  72  18]\n",
      " [382 771 115  42]\n",
      " [ 30  93  22   6]\n",
      " [  5  10   4   0]]\n",
      "\n",
      "\n",
      "The confusion matrix for the label 'Genus' is:\n",
      "[[650  96   0  52  45  32  23]\n",
      " [ 81  18   0   7   3   4   6]\n",
      " [  0   0   0   0   0   0   0]\n",
      " [ 55   7   0   4   1   1   3]\n",
      " [ 20   3   0   0   2   0   1]\n",
      " [  9   3   0   0   0   0   0]\n",
      " [ 35   6   0   0   0   1   1]]\n",
      "\n",
      "\n",
      "The confusion matrix for the label 'Species' is:\n",
      "[[ 14 103  13   5  12  38  11   6   3   5]\n",
      " [ 84 493  74  39  80 151  37  17  21  19]\n",
      " [ 14  74  16   7   6  17   7   1   3   6]\n",
      " [  7  33   9   4   7  13   6   2   2   2]\n",
      " [ 11  72   8   3   7  20   5   1   1   3]\n",
      " [ 31 174  28  21  26  62  12   7   5   7]\n",
      " [  6  50   7   1   3   9   4   1   1   3]\n",
      " [  5  17   2   3   2   6   0   1   0   1]\n",
      " [  2   8   3   2   0   4   0   0   0   0]\n",
      " [  5  31   6   2   3   5   0   0   0   1]]\n"
     ]
    }
   ],
   "source": [
    "# Question b -> vi (Confusion Matrices)\n",
    "\n",
    "# NOTE: The confusion matrices will be calculated ONLY for TEST data!!!\n",
    "\n",
    "print(\"Confusion Matrices for Linear SVC with L1 regularization and SMOTE (Binary Relevance method)\\n\")\n",
    "# Confusion matrix for label \"Family\"\n",
    "print(\"The confusion matrix for the label 'Family' is:\")\n",
    "print(metrics.confusion_matrix(y_true[:,0], y_pred3[:,0], \n",
    "                               labels=[\"Hylidae\", \"Leptodactylidae\",\n",
    "                                      \"Dendrobatidae\", \"Bufonidae\"]))\n",
    "\n",
    "# Confusion matrix for label \"Genus\"\n",
    "print(\"\\n\")\n",
    "print(\"The confusion matrix for the label 'Genus' is:\")\n",
    "print(metrics.confusion_matrix(y_true[:,1], y_pred3[:,1], \n",
    "                               labels=[\"Adenomera\", \"Ameerega\", \"Dendropsophus\"\n",
    "                                      \"Hypsiboas\", \"Leptodactylus\", \"Osteocephalus\",\n",
    "                                      \"Rhinella\", \"Scinax\"]))\n",
    "\n",
    "# Confusion matrix for label \"Species\"\n",
    "print(\"\\n\")\n",
    "print(\"The confusion matrix for the label 'Species' is:\")\n",
    "print(metrics.confusion_matrix(y_true[:,2], y_pred3[:,2], \n",
    "                               labels=[\"AdenomeraAndre\", \"AdenomeraHylaedactylus\", \n",
    "                                       \"Ameeregatrivittata\", \"HylaMinuta\",\n",
    "                                      \"HypsiboasCinerascens\", \"HypsiboasCordobae\",\n",
    "                                      \"LeptodactylusFuscus\", \"OsteocephalusOophagus\",\n",
    "                                      \"Rhinellagranulosa\", \"ScinaxRuber\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrices for SVM with Gaussian kernel (Classifier Chain method)\n",
      "\n",
      "The confusion matrix for the label 'Family' is:\n",
      "[[ 676    3    0    0]\n",
      " [   9 1301    0    0]\n",
      " [   0    0  151    0]\n",
      " [   1    0    0   18]]\n",
      "\n",
      "\n",
      "The confusion matrix for the label 'Genus' is:\n",
      "[[1219    0    0    0    0    0    0]\n",
      " [   0  151    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0]\n",
      " [   0    0    0   83    0    0    0]\n",
      " [   0    0    0    0   35    0    0]\n",
      " [   0    0    0    0    0   18    0]\n",
      " [   0    0    0    0    0    0   52]]\n",
      "\n",
      "\n",
      "The confusion matrix for the label 'Species' is:\n",
      "[[ 207    0    0    1    1    1    0    0    0    0]\n",
      " [   0 1012    0    1    0    2    0    0    0    0]\n",
      " [   0    0  151    0    0    0    0    0    0    0]\n",
      " [   1    1    0   83    0    0    0    0    0    0]\n",
      " [   0    0    0    0  131    0    0    0    0    0]\n",
      " [   0    1    0    0    2  369    0    1    0    0]\n",
      " [   0    0    0    0    1    1   83    0    0    0]\n",
      " [   0    0    0    0    1    1    0   35    0    0]\n",
      " [   0    0    0    0    1    0    0    0   18    0]\n",
      " [   0    0    0    1    0    0    0    0    0   52]]\n"
     ]
    }
   ],
   "source": [
    "# Question b -> vi (Confusion Matrices)\n",
    "\n",
    "# NOTE: The confusion matrices will be calculated ONLY for TEST data!!!\n",
    "\n",
    "print(\"Confusion Matrices for SVM with Gaussian kernel (Classifier Chain method)\\n\")\n",
    "# Confusion matrix for label \"Family\"\n",
    "print(\"The confusion matrix for the label 'Family' is:\")\n",
    "print(metrics.confusion_matrix(y_true[:,0], y_pred4[:,0], \n",
    "                               labels=[\"Hylidae\", \"Leptodactylidae\",\n",
    "                                      \"Dendrobatidae\", \"Bufonidae\"]))\n",
    "\n",
    "# Confusion matrix for label \"Genus\"\n",
    "print(\"\\n\")\n",
    "print(\"The confusion matrix for the label 'Genus' is:\")\n",
    "print(metrics.confusion_matrix(y_true[:,1], y_pred4[:,1], \n",
    "                               labels=[\"Adenomera\", \"Ameerega\", \"Dendropsophus\"\n",
    "                                      \"Hypsiboas\", \"Leptodactylus\", \"Osteocephalus\",\n",
    "                                      \"Rhinella\", \"Scinax\"]))\n",
    "\n",
    "# Confusion matrix for label \"Species\"\n",
    "print(\"\\n\")\n",
    "print(\"The confusion matrix for the label 'Species' is:\")\n",
    "print(metrics.confusion_matrix(y_true[:,2], y_pred4[:,2], \n",
    "                               labels=[\"AdenomeraAndre\", \"AdenomeraHylaedactylus\", \n",
    "                                       \"Ameeregatrivittata\", \"HylaMinuta\",\n",
    "                                      \"HypsiboasCinerascens\", \"HypsiboasCordobae\",\n",
    "                                      \"LeptodactylusFuscus\", \"OsteocephalusOophagus\",\n",
    "                                      \"Rhinellagranulosa\", \"ScinaxRuber\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Precision and Recall for SVM with Gaussian kernel (Binary Relevance method)\n",
      "\n",
      "Precision and Recall for label 'Family':\n",
      "precision = [1.         1.         0.98542274 0.99769939]\n",
      "recall = [0.94736842 1.         0.99558174 0.99312977]\n",
      "\n",
      "Precision and Recall for label 'Genus':\n",
      "precision = [0.99754501 1.         0.96511628 0.98238748 1.         0.97222222\n",
      " 1.         1.        ]\n",
      "recall = [0.99510204 1.         0.97647059 0.99603175 0.97647059 0.94594595\n",
      " 0.94736842 0.98113208]\n",
      "\n",
      "Precision and Recall for label 'Species':\n",
      "precision = [0.99519231 0.99802761 1.         0.96511628 0.95620438 0.98663102\n",
      " 1.         0.97222222 1.         1.        ]\n",
      "recall = [0.98571429 0.99704433 1.         0.97647059 1.         0.98927614\n",
      " 0.97647059 0.94594595 0.94736842 0.98113208]\n",
      "\n",
      "\n",
      "2) Precision and Recall for Linear SVC with L1 regularization (Binary Relevance method)\n",
      "\n",
      "Precision and Recall for label 'Family':\n",
      "precision = [0.         0.93661972 0.91848617 0.95789474]\n",
      "recall = [0.         0.8807947  0.92930781 0.97251908]\n",
      "\n",
      "Precision and Recall for label 'Genus':\n",
      "precision = [0.9672     0.94557823 0.9047619  0.91791045 0.98648649 0.94736842\n",
      " 0.875      0.96296296]\n",
      "recall = [0.98693878 0.9205298  0.67058824 0.97619048 0.85882353 0.48648649\n",
      " 0.73684211 0.98113208]\n",
      "\n",
      "Precision and Recall for label 'Species':\n",
      "precision = [0.91666667 0.99117647 0.92666667 0.91304348 0.91428571 0.92385787\n",
      " 0.98717949 1.         0.94736842 0.96296296]\n",
      "recall = [0.94285714 0.99605911 0.9205298  0.74117647 0.97709924 0.97587131\n",
      " 0.90588235 0.51351351 0.94736842 0.98113208]\n",
      "\n",
      "\n",
      "3) Precision and Recall for Linear SVC with L1 regularization and SMOTE (Binary Relevance method)\n",
      "\n",
      "Precision and Recall for label 'Family':\n",
      "precision = [0.         0.10328638 0.31862745 0.60804416]\n",
      "recall = [0.         0.14569536 0.28718704 0.58854962]\n",
      "\n",
      "Precision and Recall for label 'Genus':\n",
      "precision = [0.56277056 0.09677419 0.05263158 0.25054945 0.04597701 0.02985075\n",
      " 0.         0.02173913]\n",
      "recall = [0.53061224 0.1192053  0.07058824 0.22619048 0.04705882 0.05405405\n",
      " 0.         0.01886792]\n",
      "\n",
      "Precision and Recall for label 'Species':\n",
      "precision = [0.07821229 0.46729858 0.09638554 0.04597701 0.04794521 0.19076923\n",
      " 0.04878049 0.02777778 0.         0.0212766 ]\n",
      "recall = [0.06666667 0.48571429 0.10596026 0.04705882 0.05343511 0.16621984\n",
      " 0.04705882 0.02702703 0.         0.01886792]\n",
      "\n",
      "\n",
      "4) Precision and Recall for SVM with Gaussian kernel (Classifier Chain method)\n",
      "\n",
      "Precision and Recall for label 'Family':\n",
      "precision = [1.         1.         0.98542274 0.99769939]\n",
      "recall = [0.         0.8807947  0.92930781 0.97251908]\n",
      "\n",
      "Precision and Recall for label 'Genus':\n",
      "precision = [0.99754501 1.         0.96511628 0.98238748 1.         0.97222222\n",
      " 1.         1.        ]\n",
      "recall = [0.98693878 0.9205298  0.67058824 0.97619048 0.85882353 0.48648649\n",
      " 0.73684211 0.98113208]\n",
      "\n",
      "Precision and Recall for label 'Species':\n",
      "precision = [0.99519231 0.99802761 1.         0.96511628 0.95620438 0.98663102\n",
      " 1.         0.97222222 1.         1.        ]\n",
      "recall = [0.94285714 0.99605911 0.9205298  0.74117647 0.97709924 0.97587131\n",
      " 0.90588235 0.51351351 0.94736842 0.98113208]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai-maris/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Question b -> vi (Preciion and Recall)\n",
    "\n",
    "# We will calculate Precision and Recall measures for the problem.\n",
    "# In order to do this, we actually have to calculate these metrics \n",
    "# for each label!\n",
    "\n",
    "# NOTE: The measures will be calculated ONLY for TEST data!!!\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(\"1) Precision and Recall for SVM with Gaussian kernel (Binary Relevance method)\\n\")\n",
    "print(\"Precision and Recall for label 'Family':\")\n",
    "# By selecting average=None below, I print precision and recall for each class!\n",
    "print(\"precision =\", precision_score(y_true[:,0], y_pred[:,0], average=None, sample_weight=None))\n",
    "print(\"recall =\", recall_score(y_true[:,0], y_pred[:,0], average=None, sample_weight=None))\n",
    "\n",
    "print(\"\\nPrecision and Recall for label 'Genus':\")\n",
    "# By selecting average=None below, I print precision and recall for each class!\n",
    "print(\"precision =\", precision_score(y_true[:,1], y_pred[:,1], average=None, sample_weight=None))\n",
    "print(\"recall =\", recall_score(y_true[:,1], y_pred[:,1], average=None, sample_weight=None))\n",
    "\n",
    "print(\"\\nPrecision and Recall for label 'Species':\")\n",
    "# By selecting average=None below, I print precision and recall for each class!\n",
    "print(\"precision =\", precision_score(y_true[:,2], y_pred[:,2], average=None, sample_weight=None))\n",
    "print(\"recall =\", recall_score(y_true[:,2], y_pred[:,2], average=None, sample_weight=None))\n",
    "\n",
    "\n",
    "print(\"\\n\\n2) Precision and Recall for Linear SVC with L1 regularization (Binary Relevance method)\\n\")\n",
    "print(\"Precision and Recall for label 'Family':\")\n",
    "# By selecting average=None below, I print precision and recall for each class!\n",
    "print(\"precision =\", precision_score(y_true[:,0], y_pred2[:,0], average=None, sample_weight=None))\n",
    "print(\"recall =\", recall_score(y_true[:,0], y_pred2[:,0], average=None, sample_weight=None))\n",
    "\n",
    "print(\"\\nPrecision and Recall for label 'Genus':\")\n",
    "# By selecting average=None below, I print precision and recall for each class!\n",
    "print(\"precision =\", precision_score(y_true[:,1], y_pred2[:,1], average=None, sample_weight=None))\n",
    "print(\"recall =\", recall_score(y_true[:,1], y_pred2[:,1], average=None, sample_weight=None))\n",
    "\n",
    "print(\"\\nPrecision and Recall for label 'Species':\")\n",
    "# By selecting average=None below, I print precision and recall for each class!\n",
    "print(\"precision =\", precision_score(y_true[:,2], y_pred2[:,2], average=None, sample_weight=None))\n",
    "print(\"recall =\", recall_score(y_true[:,2], y_pred2[:,2], average=None, sample_weight=None))\n",
    "\n",
    "\n",
    "print(\"\\n\\n3) Precision and Recall for Linear SVC with L1 regularization and SMOTE (Binary Relevance method)\\n\")\n",
    "print(\"Precision and Recall for label 'Family':\")\n",
    "# By selecting average=None below, I print precision and recall for each class!\n",
    "print(\"precision =\", precision_score(y_true[:,0], y_pred3[:,0], average=None, sample_weight=None))\n",
    "print(\"recall =\", recall_score(y_true[:,0], y_pred3[:,0], average=None, sample_weight=None))\n",
    "\n",
    "print(\"\\nPrecision and Recall for label 'Genus':\")\n",
    "# By selecting average=None below, I print precision and recall for each class!\n",
    "print(\"precision =\", precision_score(y_true[:,1], y_pred3[:,1], average=None, sample_weight=None))\n",
    "print(\"recall =\", recall_score(y_true[:,1], y_pred3[:,1], average=None, sample_weight=None))\n",
    "\n",
    "print(\"\\nPrecision and Recall for label 'Species':\")\n",
    "# By selecting average=None below, I print precision and recall for each class!\n",
    "print(\"precision =\", precision_score(y_true[:,2], y_pred3[:,2], average=None, sample_weight=None))\n",
    "print(\"recall =\", recall_score(y_true[:,2], y_pred3[:,2], average=None, sample_weight=None))\n",
    "\n",
    "\n",
    "print(\"\\n\\n4) Precision and Recall for SVM with Gaussian kernel (Classifier Chain method)\\n\")\n",
    "print(\"Precision and Recall for label 'Family':\")\n",
    "# By selecting average=None below, I print precision and recall for each class!\n",
    "print(\"precision =\", precision_score(y_true[:,0], y_pred4[:,0], average=None, sample_weight=None))\n",
    "print(\"recall =\", recall_score(y_true[:,0], y_pred2[:,0], average=None, sample_weight=None))\n",
    "\n",
    "print(\"\\nPrecision and Recall for label 'Genus':\")\n",
    "# By selecting average=None below, I print precision and recall for each class!\n",
    "print(\"precision =\", precision_score(y_true[:,1], y_pred4[:,1], average=None, sample_weight=None))\n",
    "print(\"recall =\", recall_score(y_true[:,1], y_pred2[:,1], average=None, sample_weight=None))\n",
    "\n",
    "print(\"\\nPrecision and Recall for label 'Species':\")\n",
    "# By selecting average=None below, I print precision and recall for each class!\n",
    "print(\"precision =\", precision_score(y_true[:,2], y_pred4[:,2], average=None, sample_weight=None))\n",
    "print(\"recall =\", recall_score(y_true[:,2], y_pred2[:,2], average=None, sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
