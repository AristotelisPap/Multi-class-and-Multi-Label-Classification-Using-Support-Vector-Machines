{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 559 - Homework 6\n",
    "\n",
    "## Name: Aristotelis-Angelos Papadopoulos\n",
    "## USC ID: 3804-2945-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question a\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a dataframe with the dataset\n",
    "dataset = pd.read_csv('Frogs_MFCCs.csv', sep = \",\", header = 'infer')\n",
    "\n",
    "# Take 70% of the data for training and the rest for test\n",
    "train_set, test_set = train_test_split(dataset, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question b -> i**\n",
    "\n",
    "The Exact Match ratio and the Hamming loss methods for evaluating multi-label \n",
    "classification problems are well presented in the paper \"A Literature Survey on Algorithms for Multi-label\n",
    "Learning\" by Mohammad S. Sorower which can be found [here](https://www.researchgate.net/profile/Mohammad_Sorower/publication/266888594_A_Literature_Survey_on_Algorithms_for_Multi-label_Learning/links/58d1864392851cf4f8f4b72a/A-Literature-Survey-on-Algorithms-for-Multi-label-Learning.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question b -> ii\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# As the problem suggests, we will train a classifier for each label.\n",
    "# So, let us first extract the columns corresponding to these 3 labels.\n",
    "family_train = train_set['Family'] # Label 1\n",
    "genus_train = train_set['Genus'] # Label 2\n",
    "species_train = train_set['Species'] # Label 3\n",
    "features_train = train_set.iloc[:,0:22] \n",
    "\n",
    "family_test = test_set['Family'] # Label 1\n",
    "genus_test = test_set['Genus'] # Label 2\n",
    "species_test = test_set['Species'] # Label 3\n",
    "features_test = test_set.iloc[:,0:22] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 10000.0\n",
      "The best gamma is gamma= 2.3\n",
      "The CV error for those values is:  0.0057587362197410565\n",
      "For the label 'Family', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  100.0 %\n",
      "The cross-validation accuracy of the model is:  99.42412637802589 %\n",
      "The test accuracy for the label 'Family' is:  99.21259842519686 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> ii (Continue)\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Family\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the parameters C and gamma. Since, I did not have a problem\n",
    "# with the computational power, I created a grid of 400 points and picked\n",
    "# the values that resulted in the lowest CV error.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=10) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "gamma_range = np.linspace(0.1, 4, 40) # gamma ranges from 0.1 to 4 with linear increment\n",
    "\n",
    "\n",
    "for C1 in C_range:\n",
    "    for gamm in gamma_range: \n",
    "        # I will use 10-fold cross validation and I will shuffle\n",
    "        # the data before each split!\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "        skf.get_n_splits(features_train)\n",
    "        # Initialize a list in order to get the errors from 10-fold CV\n",
    "        list_10fold = []\n",
    "        for train_index, test_index in skf.split(features_train,family_train):\n",
    "            X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "            y_train, y_test = family_train.values[train_index], family_train.values[test_index]\n",
    "            \n",
    "            # Now, I will fit my SVM model in the k-1 folds!\n",
    "            classif = SVC(C=C1, kernel='rbf', gamma=gamm, decision_function_shape='ovr')\n",
    "            classif.fit(X_train, y_train)\n",
    "            list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "        # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "        ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(10,40) # Indexes are (C, gamma)\n",
    "# Take the indexes of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 10 different values of C\n",
    "# and the 40 different values of gamma!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The best gamma is gamma=\", gamma_range[ind[1]])\n",
    "print(\"The CV error for those values is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best values \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = SVC(C=C_range[ind[0]], kernel='rbf', gamma=gamma_range[ind[1]], decision_function_shape='ovr')\n",
    "classif.fit(features_train, family_train)\n",
    "\n",
    "print(\"For the label 'Family', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, family_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Family' is: \", classif.score(features_test, family_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 10000.0\n",
      "The best gamma is gamma= 2.1999999999999997\n",
      "The CV error for those values is:  0.008136772350812272\n",
      "For the label 'Genus', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  100.0 %\n",
      "The cross-validation accuracy of the model is:  99.18632276491877 %\n",
      "The test accuracy for the label 'Genus' is:  98.98100972672533 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> ii (Continue)\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Genus\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the parameters C and gamma. Since, I did not have a problem\n",
    "# with the computational power, I created a grid of 400 points and picked\n",
    "# the values that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=10) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "gamma_range = np.linspace(0.1, 4, 40) # gamma ranges from 0.1 to 4 with linear increment\n",
    "\n",
    "\n",
    "for C1 in C_range:\n",
    "    for gamm in gamma_range: \n",
    "        # I will use 10-fold cross validation and I will shuffle\n",
    "        # the data before each split!\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "        skf.get_n_splits(features_train)\n",
    "        # Initialize a list in order to get the errors from 10-fold CV\n",
    "        list_10fold = []\n",
    "        for train_index, test_index in skf.split(features_train,genus_train):\n",
    "            X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "            y_train, y_test = genus_train.values[train_index], genus_train.values[test_index]\n",
    "            \n",
    "            # Now, I will fit my SVM model in the k-1 folds!\n",
    "            classif = SVC(C=C1, kernel='rbf', gamma=gamm, decision_function_shape='ovr')\n",
    "            classif.fit(X_train, y_train)\n",
    "            list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "        # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "        ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(10,40) # Indexes are (C, gamma)\n",
    "# Take the indexes of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 10 different values of C\n",
    "# and the 40 different values of gamma!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The best gamma is gamma=\", gamma_range[ind[1]])\n",
    "print(\"The CV error for those values is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best values \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = SVC(C=C_range[ind[0]], kernel='rbf', gamma=gamma_range[ind[1]], decision_function_shape='ovr')\n",
    "classif.fit(features_train, genus_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Genus', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, genus_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Genus' is: \", classif.score(features_test, genus_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 100.0\n",
      "The best gamma is gamma= 2.3\n",
      "The CV error for those values is:  0.008339586823110168\n",
      "For the label 'Species', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  100.0 %\n",
      "The cross-validation accuracy of the model is:  99.16604131768898 %\n",
      "The test accuracy for the label 'Species' is:  98.98100972672533 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> ii (Continue)\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Species\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the parameters C and gamma. Since, I did not have a problem\n",
    "# with the computational power, I created a grid of 400 points and picked\n",
    "# the values that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=10) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "gamma_range = np.linspace(0.1, 4, 40) # gamma ranges from 0.1 to 4 with linear increment\n",
    "\n",
    "\n",
    "for C1 in C_range:\n",
    "    for gamm in gamma_range: \n",
    "        # I will use 10-fold cross validation and I will shuffle\n",
    "        # the data before each split!\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "        skf.get_n_splits(features_train)\n",
    "        # Initialize a list in order to get the errors from 10-fold CV\n",
    "        list_10fold = []\n",
    "        for train_index, test_index in skf.split(features_train,species_train):\n",
    "            X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "            y_train, y_test = species_train.values[train_index], species_train.values[test_index]\n",
    "            \n",
    "            # Now, I will fit my SVM model in the k-1 folds!\n",
    "            classif = SVC(C=C1, kernel='rbf', gamma=gamm, decision_function_shape='ovr')\n",
    "            classif.fit(X_train, y_train)\n",
    "            list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "        # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "        ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(10,40) # Indexes are (C, gamma)\n",
    "# Take the indexes of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 10 different values of C\n",
    "# and the 40 different values of gamma!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The best gamma is gamma=\", gamma_range[ind[1]])\n",
    "print(\"The CV error for those values is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best values \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = SVC(C=C_range[ind[0]], kernel='rbf', gamma=gamma_range[ind[1]], decision_function_shape='ovr')\n",
    "classif.fit(features_train, species_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Species', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, species_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Species' is: \", classif.score(features_test, species_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, using Binary Relevance with SVM with Gaussian Kernel, we have: \n",
      "\n",
      "The Exact Match score is: 0.9856415006947661\n",
      "The Hamming Loss is: 0.00941794040450826\n"
     ]
    }
   ],
   "source": [
    "# Question b -> ii (Continue)\n",
    "\n",
    "# Classifier for label 'Family'\n",
    "classif1 = SVC(C=10000, decision_function_shape='ovr', gamma=2.3, kernel='rbf')\n",
    "classif1.fit(features_train, family_train)\n",
    "\n",
    "# Classifier for label 'Genus'\n",
    "classif2 = SVC(C=10000, decision_function_shape='ovr', gamma=2.2, kernel='rbf')\n",
    "classif2.fit(features_train, genus_train)\n",
    "\n",
    "# Classifier for label 'Species'\n",
    "classif3 = SVC(C=100, decision_function_shape='ovr', gamma=2.3, kernel='rbf')\n",
    "classif3.fit(features_train, species_train)\n",
    "\n",
    "# Now, I will concatenate the 3 predicted labels into 1 numpy array\n",
    "y_pred = np.concatenate((np.expand_dims(classif1.predict(features_test), axis=1), \n",
    "                         np.expand_dims(classif2.predict(features_test), axis=1),\n",
    "                         np.expand_dims(classif3.predict(features_test), axis=1)), axis=1)\n",
    "\n",
    "# Now, I will convert the true labels into a numpy array\n",
    "y_true = np.concatenate((np.expand_dims(np.asarray(family_test), axis=1), \n",
    "                         np.expand_dims(np.asarray(genus_test), axis=1),\n",
    "                         np.expand_dims(np.asarray(species_test), axis=1)), axis=1)\n",
    "\n",
    "# Calculate the Exact Match score and the Hamming Loss\n",
    "EMscore = 0\n",
    "count_Ham = 0\n",
    "for i in range(0,y_true.shape[0]):\n",
    "    count_EM = 0\n",
    "    for j in range(0,y_true.shape[1]):\n",
    "        if y_true[i][j]==y_pred[i][j]:\n",
    "            count_EM += 1\n",
    "        else:\n",
    "            count_Ham += 1\n",
    "    if count_EM==3:\n",
    "        EMscore += 1\n",
    "EMscore = EMscore / y_true.shape[0]\n",
    "Hamming_Loss = count_Ham / (y_true.shape[0] * y_true.shape[1])\n",
    "            \n",
    "print(\"So, using Binary Relevance with SVM with Gaussian Kernel, we have:\",\"\\n\")\n",
    "print(\"The Exact Match score is:\", EMscore)\n",
    "print(\"The Hamming Loss is:\", Hamming_Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 2.848035868435802\n",
      "The CV error for this value is:  0.061545746729767446\n",
      "For the label 'Family', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  93.86417791898332 %\n",
      "The cross-validation accuracy of the model is:  93.84542532702326 %\n",
      "The test accuracy for the label 'Family' is:  93.42288096340899 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iii\n",
    "\n",
    "# In this question, we will apply the L1-penalized SVM algorithm.\n",
    "# Note that we will not normalize the attributes since they were\n",
    "# already normalized!\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Family\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the L1 regularization coefficient. \n",
    "# Since I did not have a problem with the computational power, I created a grid \n",
    "# of 100 points and picked the value that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=100) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "\n",
    "for C1 in C_range: \n",
    "    # I will use 10-fold cross validation and I will shuffle\n",
    "    # the data before each split!\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "    skf.get_n_splits(features_train)\n",
    "    # Initialize a list in order to get the errors from 10-fold CV\n",
    "    list_10fold = []\n",
    "    for train_index, test_index in skf.split(features_train,family_train):\n",
    "        X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "        y_train, y_test = family_train.values[train_index], family_train.values[test_index]\n",
    "            \n",
    "        # Now, I will fit my SVM model in the k-1 folds!\n",
    "        classif = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, C=C1, multi_class='ovr')\n",
    "        classif.fit(X_train, y_train)\n",
    "        list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "    # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "    ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(100,1) # Indexes are (C)\n",
    "# Take the index of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 100 different values of C!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The CV error for this value is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best value \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=C_range[ind[0]], multi_class='ovr')\n",
    "classif.fit(features_train, family_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Family', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, family_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Family' is: \", classif.score(features_test, family_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 18738.174228603868\n",
      "The CV error for this value is:  0.04605887310243938\n",
      "For the label 'Genus', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  95.59173947577443 %\n",
      "The cross-validation accuracy of the model is:  95.39411268975606 %\n",
      "The test accuracy for the label 'Genus' is:  95.83140342751274 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iii (Continue)\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Genus\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the L1 regularization coefficient. \n",
    "# Since I did not have a problem with the computational power, I created a grid \n",
    "# of 100 points and picked the value that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=100) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "\n",
    "for C1 in C_range: \n",
    "    # I will use 10-fold cross validation and I will shuffle\n",
    "    # the data before each split!\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "    skf.get_n_splits(features_train)\n",
    "    # Initialize a list in order to get the errors from 10-fold CV\n",
    "    list_10fold = []\n",
    "    for train_index, test_index in skf.split(features_train,genus_train):\n",
    "        X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "        y_train, y_test = genus_train.values[train_index], genus_train.values[test_index]\n",
    "            \n",
    "        # Now, I will fit my SVM model in the k-1 folds!\n",
    "        classif = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, C=C1, multi_class='ovr')\n",
    "        classif.fit(X_train, y_train)\n",
    "        list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "    # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "    ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(100,1) # Indexes are (C)\n",
    "# Take the index of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 100 different values of C!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The CV error for this value is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best value \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=C_range[ind[0]], multi_class='ovr')\n",
    "classif.fit(features_train, genus_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Genus', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, genus_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Genus' is: \", classif.score(features_test, genus_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 1519.9110829529332\n",
      "The CV error for this value is:  0.03931985181947569\n",
      "For the label 'Species', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  96.44559173947577 %\n",
      "The cross-validation accuracy of the model is:  96.06801481805243 %\n",
      "The test accuracy for the label 'Species' is:  96.34089856415007 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iii (Continue)\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Species\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the L1 regularization coefficient. \n",
    "# Since I did not have a problem with the computational power, I created a grid \n",
    "# of 100 points and picked the value that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=100) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "\n",
    "for C1 in C_range: \n",
    "    # I will use 10-fold cross validation and I will shuffle\n",
    "    # the data before each split!\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "    skf.get_n_splits(features_train)\n",
    "    # Initialize a list in order to get the errors from 10-fold CV\n",
    "    list_10fold = []\n",
    "    for train_index, test_index in skf.split(features_train,species_train):\n",
    "        X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "        y_train, y_test = species_train.values[train_index], species_train.values[test_index]\n",
    "            \n",
    "        # Now, I will fit my SVM model in the k-1 folds!\n",
    "        classif = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, C=C1, multi_class='ovr')\n",
    "        classif.fit(X_train, y_train)\n",
    "        list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "    # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "    ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(100,1) # Indexes are (C)\n",
    "# Take the index of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 100 different values of C!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The CV error for this value is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best value \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=C_range[ind[0]], multi_class='ovr')\n",
    "classif.fit(features_train, species_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Species', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, species_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Species' is: \", classif.score(features_test, species_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, using Binary Relevance with L1-penalized SVM, we have: \n",
      "\n",
      "The Exact Match score is: 0.9170912459471978\n",
      "The Hamming Loss is: 0.04801605681642736\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iii (Continue)\n",
    "\n",
    "# Classifier for label 'Family'\n",
    "classif4 = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=2.848, multi_class='ovr')\n",
    "classif4.fit(features_train, family_train)\n",
    "\n",
    "# Classifier for label 'Genus'\n",
    "classif5 = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=18738.2, multi_class='ovr')\n",
    "classif5.fit(features_train, genus_train)\n",
    "\n",
    "# Classifier for label 'Species'\n",
    "classif6 = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=1519.9, multi_class='ovr')\n",
    "classif6.fit(features_train, species_train)\n",
    "\n",
    "# Now, I will concatenate the 3 predicted labels into 1 numpy array\n",
    "y_pred2 = np.concatenate((np.expand_dims(classif4.predict(features_test), axis=1), \n",
    "                         np.expand_dims(classif5.predict(features_test), axis=1),\n",
    "                         np.expand_dims(classif6.predict(features_test), axis=1)), axis=1)\n",
    "\n",
    "# Now, I will convert the true labels into a numpy array\n",
    "y_true = np.concatenate((np.expand_dims(np.asarray(family_test), axis=1), \n",
    "                         np.expand_dims(np.asarray(genus_test), axis=1),\n",
    "                         np.expand_dims(np.asarray(species_test), axis=1)), axis=1)\n",
    "\n",
    "# Calculate the Exact Match score and the Hamming Loss\n",
    "EMscore = 0\n",
    "count_Ham = 0\n",
    "for i in range(0,y_true.shape[0]):\n",
    "    count_EM = 0\n",
    "    for j in range(0,y_true.shape[1]):\n",
    "        if y_true[i][j]==y_pred2[i][j]:\n",
    "            count_EM += 1\n",
    "        else:\n",
    "            count_Ham += 1\n",
    "    if count_EM==3:\n",
    "        EMscore += 1\n",
    "EMscore = EMscore / y_true.shape[0]\n",
    "Hamming_Loss = count_Ham / (y_true.shape[0] * y_true.shape[1])\n",
    "            \n",
    "print(\"So, using Binary Relevance with L1-penalized SVM, we have:\",\"\\n\")\n",
    "print(\"The Exact Match score is:\", EMscore)\n",
    "print(\"The Hamming Loss is:\", Hamming_Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 18738.174228603868\n",
      "The CV error for this value is:  0.07843023762711585\n",
      "For the label 'Family' using SMOTE, we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  93.88403494837172 %\n",
      "The cross-validation accuracy of the model is:  92.15697623728842 %\n",
      "The test accuracy for the label 'Family' is:  93.23761000463178 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iv\n",
    "\n",
    "# In this question, we are asked to use SMOTE in order to remedy\n",
    "# the class imbalance appearing in the dataset.\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# In this question, we will apply the L1-penalized SVM algorithm.\n",
    "# Note that we will not normalize the attributes since they were\n",
    "# already normalized!\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Family\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the L1 regularization coefficient. \n",
    "# Since I did not have a problem with the computational power, I created a grid \n",
    "# of 100 points and picked the value that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=100) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "\n",
    "for C1 in C_range: \n",
    "    # I will use 10-fold cross validation and I will shuffle\n",
    "    # the data before each split!\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "    skf.get_n_splits(features_train)\n",
    "    # Initialize a list in order to get the errors from 10-fold CV\n",
    "    list_10fold = []\n",
    "    for train_index, test_index in skf.split(features_train,family_train):\n",
    "        X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "        y_train, y_test = family_train.values[train_index], family_train.values[test_index]\n",
    "        \n",
    "        # Use SMOTE on the training data\n",
    "        sm = SMOTE(random_state=12, ratio = 'all')\n",
    "        X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "            \n",
    "        # Now, I will fit my SVM model in the k-1 folds!\n",
    "        classif = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, C=C1, multi_class='ovr')\n",
    "        classif.fit(X_train_res, y_train_res)\n",
    "        list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "    # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "    ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(100,1) # Indexes are (C)\n",
    "# Take the index of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 100 different values of C!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The CV error for this value is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best value \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=C_range[ind[0]], multi_class='ovr')\n",
    "classif.fit(features_train, family_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Family' using SMOTE, we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, family_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Family' is: \", classif.score(features_test, family_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 4.328761281083062\n",
      "The CV error for this value is:  0.08538843580758301\n",
      "For the label 'Genus' using SMOTE, we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  95.35345512311359 %\n",
      "The cross-validation accuracy of the model is:  91.46115641924169 %\n",
      "The test accuracy for the label 'Genus' is:  95.78508568781844 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iv (Continue)\n",
    "\n",
    "# In this question, we are asked to use SMOTE in order to remedy\n",
    "# the class imbalance appearing in the dataset.\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Genus\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the L1 regularization coefficient. \n",
    "# Since I did not have a problem with the computational power, I created a grid \n",
    "# of 100 points and picked the value that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=100) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "\n",
    "for C1 in C_range: \n",
    "    # I will use 10-fold cross validation and I will shuffle\n",
    "    # the data before each split!\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "    skf.get_n_splits(features_train)\n",
    "    # Initialize a list in order to get the errors from 10-fold CV\n",
    "    list_10fold = []\n",
    "    for train_index, test_index in skf.split(features_train,genus_train):\n",
    "        X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "        y_train, y_test = genus_train.values[train_index], genus_train.values[test_index]\n",
    "        \n",
    "        # Use SMOTE on the training data\n",
    "        sm = SMOTE(random_state=12, ratio = 'all')\n",
    "        X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "            \n",
    "        # Now, I will fit my SVM model in the k-1 folds!\n",
    "        classif = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, C=C1, multi_class='ovr')\n",
    "        classif.fit(X_train_res, y_train_res)\n",
    "        list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "    # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "    ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(100,1) # Indexes are (C)\n",
    "# Take the index of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 100 different values of C!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The CV error for this value is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best value \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=C_range[ind[0]], multi_class='ovr')\n",
    "classif.fit(features_train, genus_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Genus' using SMOTE, we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, genus_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Genus' is: \", classif.score(features_test, genus_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 6.5793322465756825\n",
      "The CV error for this value is:  0.04227783612861772\n",
      "For the label 'Species' using SMOTE, we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  96.36616362192217 %\n",
      "The cross-validation accuracy of the model is:  95.77221638713823 %\n",
      "The test accuracy for the label 'Species' is:  96.29458082445576 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iv (Continue)\n",
    "\n",
    "# In this question, we are asked to use SMOTE in order to remedy\n",
    "# the class imbalance appearing in the dataset.\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Species\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the L1 regularization coefficient. \n",
    "# Since I did not have a problem with the computational power, I created a grid \n",
    "# of 100 points and picked the value that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=100) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "\n",
    "for C1 in C_range: \n",
    "    # I will use 10-fold cross validation and I will shuffle\n",
    "    # the data before each split!\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "    skf.get_n_splits(features_train)\n",
    "    # Initialize a list in order to get the errors from 10-fold CV\n",
    "    list_10fold = []\n",
    "    for train_index, test_index in skf.split(features_train,species_train):\n",
    "        X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "        y_train, y_test = species_train.values[train_index], species_train.values[test_index]\n",
    "        \n",
    "        # Use SMOTE on the training data\n",
    "        sm = SMOTE(random_state=12, ratio = 'all')\n",
    "        X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "            \n",
    "        # Now, I will fit my SVM model in the k-1 folds!\n",
    "        classif = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, C=C1, multi_class='ovr')\n",
    "        classif.fit(X_train_res, y_train_res)\n",
    "        list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "    # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "    ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(100,1) # Indexes are (C)\n",
    "# Take the index of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 100 different values of C!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The CV error for this value is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best value \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=C_range[ind[0]], multi_class='ovr')\n",
    "classif.fit(features_train, species_train)\n",
    "\n",
    "\n",
    "print(\"For the label 'Species' using SMOTE, we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, species_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Species' is: \", classif.score(features_test, species_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, using Binary Relevance with L1-penalized SVM and the SMOTE technique on the training dataset, we have: \n",
      "\n",
      "The Exact Match score is: 0.8703103288559518\n",
      "The Hamming Loss is: 0.06855025474756832\n"
     ]
    }
   ],
   "source": [
    "# Question b -> iv (Continue)\n",
    "\n",
    "# Use SMOTE\n",
    "X_train_res7, y_train_res7 = sm.fit_sample(features_train, family_train)\n",
    "\n",
    "# Classifier for label 'Family'\n",
    "classif7 = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=18738.2, multi_class='ovr')\n",
    "classif7.fit(X_train_res7, y_train_res7)\n",
    "\n",
    "# Use SMOTE\n",
    "X_train_res8, y_train_res8 = sm.fit_sample(features_train, genus_train)\n",
    "\n",
    "# Classifier for label 'Genus'\n",
    "classif8 = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=4.3, multi_class='ovr')\n",
    "classif8.fit(X_train_res8, y_train_res8)\n",
    "\n",
    "# Use SMOTE\n",
    "X_train_res9, y_train_res9 = sm.fit_sample(features_train, species_train)\n",
    "\n",
    "# Classifier for label 'Species'\n",
    "classif9 = LinearSVC(penalty='l1',loss='squared_hinge', dual=False, C=6.6, multi_class='ovr')\n",
    "classif9.fit(X_train_res9, y_train_res9)\n",
    "\n",
    "# Now, I will concatenate the 3 predicted labels into 1 numpy array\n",
    "y_pred3 = np.concatenate((np.expand_dims(classif7.predict(features_test), axis=1), \n",
    "                         np.expand_dims(classif8.predict(features_test), axis=1),\n",
    "                         np.expand_dims(classif9.predict(features_test), axis=1)), axis=1)\n",
    "\n",
    "# Now, I will convert the true labels into a numpy array\n",
    "y_true = np.concatenate((np.expand_dims(np.asarray(family_test), axis=1), \n",
    "                         np.expand_dims(np.asarray(genus_test), axis=1),\n",
    "                         np.expand_dims(np.asarray(species_test), axis=1)), axis=1)\n",
    "\n",
    "# Calculate the Exact Match score and the Hamming Loss\n",
    "EMscore = 0\n",
    "count_Ham = 0\n",
    "for i in range(0,y_true.shape[0]):\n",
    "    count_EM = 0\n",
    "    for j in range(0,y_true.shape[1]):\n",
    "        if y_true[i][j]==y_pred3[i][j]:\n",
    "            count_EM += 1\n",
    "        else:\n",
    "            count_Ham += 1\n",
    "    if count_EM==3:\n",
    "        EMscore += 1\n",
    "EMscore = EMscore / y_true.shape[0]\n",
    "Hamming_Loss = count_Ham / (y_true.shape[0] * y_true.shape[1])\n",
    "            \n",
    "print(\"So, using Binary Relevance with L1-penalized SVM and the SMOTE technique on the training dataset, we have:\",\"\\n\")\n",
    "print(\"The Exact Match score is:\", EMscore)\n",
    "print(\"The Hamming Loss is:\", Hamming_Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions for Binary Relevance\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "        **Classification Methods**\n",
    "        </td>\n",
    "        <td>\n",
    "        **Exact Match score**\n",
    "        </td>\n",
    "        <td>\n",
    "        **Hamming Loss**\n",
    "        </td>\n",
    "\n",
    "    </tr>\n",
    "        <td>\n",
    "        SVM with <br />\n",
    "        Gaussian kernel\n",
    "        </td>\n",
    "        <td>\n",
    "        0.985\n",
    "        </td>\n",
    "        <td>\n",
    "        0.0094\n",
    "        </td>\n",
    "    <tr>\n",
    "    </tr>\n",
    "        <td>\n",
    "        SVM with <br />\n",
    "        L1 regularization\n",
    "        </td>\n",
    "        <td>\n",
    "        0.917\n",
    "        </td>\n",
    "        <td>\n",
    "        0.048\n",
    "        </td>\n",
    "    <tr>\n",
    "        <td>\n",
    "        SVM with <br />\n",
    "        L1 regularization <br />\n",
    "        and SMOTE\n",
    "        </td>\n",
    "        <td>\n",
    "        0.87\n",
    "        </td>\n",
    "        <td>\n",
    "        0.0685\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be observed from the above table, the SVM with Gaussian kernel algorithm achieves the highest Exact Match score and the lowest Hamming Loss compared to the other two algorithms. This is happening since this algorithm creates nonlinear decision boundaries among the classes and the data are not fully linearly separable.\n",
    "\n",
    "However, it should be noted that the other two algorithms achieved good scores too even though the SMOTE approach required a lot of computational power in order to apply the 10-fold CV approach for 100 values of the regularization parameter C. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Chain method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 1000.0\n",
      "The best gamma is gamma= 2.5\n",
      "The CV error for those values is:  0.006553543766363435\n",
      "For the label 'Family', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  100.0 %\n",
      "The cross-validation accuracy of the model is:  99.34464562336366 %\n",
      "The test accuracy for the label 'Family' is:  99.39786938397405 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question b -> v\n",
    "\n",
    "# At first, we have to train our algorithm for the 1st label, let's \n",
    "# say \"Family\" as we also did in the Binary Relevance case!\n",
    "\n",
    "# In this cell, we will train a SVM for the label \"Family\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the parameters C and gamma. Since, I did not have a problem\n",
    "# with the computational power, I created a grid of 400 points and picked\n",
    "# the values that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=10) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "gamma_range = np.linspace(0.1, 4, 40) # gamma ranges from 0.1 to 4 with linear increment\n",
    "\n",
    "\n",
    "for C1 in C_range:\n",
    "    for gamm in gamma_range: \n",
    "        # I will use 10-fold cross validation and I will shuffle\n",
    "        # the data before each split!\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "        skf.get_n_splits(features_train)\n",
    "        # Initialize a list in order to get the errors from 10-fold CV\n",
    "        list_10fold = []\n",
    "        for train_index, test_index in skf.split(features_train,family_train):\n",
    "            X_train, X_test = features_train.values[train_index], features_train.values[test_index]\n",
    "            y_train, y_test = family_train.values[train_index], family_train.values[test_index]\n",
    "            \n",
    "            # Now, I will fit my SVM model in the k-1 folds!\n",
    "            classif = SVC(C=C1, kernel='rbf', gamma=gamm, decision_function_shape='ovr')\n",
    "            classif.fit(X_train, y_train)\n",
    "            list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "        # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "        ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(10,40) # Indexes are (C, gamma)\n",
    "# Take the indexes of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 10 different values of C\n",
    "# and the 40 different values of gamma!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The best gamma is gamma=\", gamma_range[ind[1]])\n",
    "print(\"The CV error for those values is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best values \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = SVC(C=C_range[ind[0]], kernel='rbf', gamma=gamma_range[ind[1]], decision_function_shape='ovr')\n",
    "classif.fit(features_train, family_train)\n",
    "\n",
    "print(\"For the label 'Family', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train, family_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")\n",
    "# And now, we will test it on the test set\n",
    "print(\"The test accuracy for the label 'Family' is: \", classif.score(features_test, family_test)*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 1000000.0\n",
      "The best gamma is gamma= 0.8999999999999999\n",
      "The CV error for those values is:  0.003373117840297446\n",
      "For the label 'Genus', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  100.0 %\n",
      "The cross-validation accuracy of the model is:  99.66268821597025 %\n"
     ]
    }
   ],
   "source": [
    "# Question b -> v (Continue)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Since the method \"fit\" cannot handle strings I will rename my classes.\n",
    "le.fit(family_train)\n",
    "family_train1 = le.transform(family_train)\n",
    "\n",
    "# Now, I have to add the label for Family as a \n",
    "# new column (feature) in my dataset and train the next classifier\n",
    "# for this augmented set of features!\n",
    "features_train1 = np.concatenate((features_train, \n",
    "                      np.expand_dims(family_train1, axis=1)), axis=1)\n",
    "#Convert it into a dataframe\n",
    "features_train_augment_1 = pd.DataFrame(data=features_train1,index=None,columns=None)\n",
    "\n",
    "# Now, I have to train the next classifier for the \n",
    "# augmented set of features!\n",
    "\n",
    "# We will train a SVM for the label \"Genus\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the parameters C and gamma. Since, I did not have a problem\n",
    "# with the computational power, I created a grid of 400 points and picked\n",
    "# the values that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=10) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "gamma_range = np.linspace(0.1, 4, 40) # gamma ranges from 0.1 to 4 with linear increment\n",
    "\n",
    "\n",
    "for C1 in C_range:\n",
    "    for gamm in gamma_range: \n",
    "        # I will use 10-fold cross validation and I will shuffle\n",
    "        # the data before each split!\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "        skf.get_n_splits(features_train_augment_1)\n",
    "        # Initialize a list in order to get the errors from 10-fold CV\n",
    "        list_10fold = []\n",
    "        for train_index, test_index in skf.split(features_train_augment_1,genus_train):\n",
    "            X_train, X_test = features_train_augment_1.values[train_index], features_train_augment_1.values[test_index]\n",
    "            y_train, y_test = genus_train.values[train_index], genus_train.values[test_index]\n",
    "            \n",
    "            # Now, I will fit my SVM model in the k-1 folds!\n",
    "            classif = SVC(C=C1, kernel='rbf', gamma=gamm, decision_function_shape='ovr')\n",
    "            classif.fit(X_train, y_train)\n",
    "            list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "        # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "        ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(10,40) # Indexes are (C, gamma)\n",
    "# Take the indexes of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 10 different values of C\n",
    "# and the 40 different values of gamma!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The best gamma is gamma=\", gamma_range[ind[1]])\n",
    "print(\"The CV error for those values is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best values \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = SVC(C=C_range[ind[0]], kernel='rbf', gamma=gamma_range[ind[1]], decision_function_shape='ovr')\n",
    "classif.fit(features_train_augment_1, genus_train)\n",
    "\n",
    "print(\"For the label 'Genus', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train_augment_1, genus_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum CV error happens for : C = 100.0\n",
      "The best gamma is gamma= 0.5\n",
      "The CV error for those values is:  0.0007877944257230762\n",
      "For the label 'Species', we have: \n",
      "\n",
      "The accuracy of the model on the training set is:  100.0 %\n",
      "The cross-validation accuracy of the model is:  99.92122055742769 %\n"
     ]
    }
   ],
   "source": [
    "# Question b -> v (Continue)\n",
    "\n",
    "# Since the method \"fit\" cannot handle strings I will rename my classes.\n",
    "le.fit(genus_train)\n",
    "genus_train1 = le.transform(genus_train)\n",
    "\n",
    "# Now, I have to add the label for Genus as a \n",
    "# new column (feature) in my already augmented dataset \n",
    "# and train the next classifier for this augmented set of features!\n",
    "features_train2 = np.concatenate((features_train1, \n",
    "                      np.expand_dims(genus_train1, axis=1)), axis=1)\n",
    "#Convert it into a dataframe\n",
    "features_train_augment_2 = pd.DataFrame(data=features_train2,index=None,columns=None)\n",
    "\n",
    "# Now, I have to train the next classifier for the \n",
    "# augmented set of features!\n",
    "\n",
    "# We will train a SVM for the label \"Species\".\n",
    "# As suggested, we will use 10-fold cross validation in order to choose the \n",
    "# best values for the parameters C and gamma. Since, I did not have a problem\n",
    "# with the computational power, I created a grid of 400 points and picked\n",
    "# the values that resulted in the lowest CV error.\n",
    "\n",
    "# In this list, I will save the different CV errors that I am going to compare.\n",
    "ave_CV_errors = []\n",
    "\n",
    "\n",
    "C_range = np.logspace(-3, 6, num=10) # C ranges from 10^-3 to 10^6 with a log increment \n",
    "gamma_range = np.linspace(0.1, 4, 40) # gamma ranges from 0.1 to 4 with linear increment\n",
    "\n",
    "\n",
    "for C1 in C_range:\n",
    "    for gamm in gamma_range: \n",
    "        # I will use 10-fold cross validation and I will shuffle\n",
    "        # the data before each split!\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "        skf.get_n_splits(features_train_augment_2)\n",
    "        # Initialize a list in order to get the errors from 10-fold CV\n",
    "        list_10fold = []\n",
    "        for train_index, test_index in skf.split(features_train_augment_2,species_train):\n",
    "            X_train, X_test = features_train_augment_2.values[train_index], features_train_augment_2.values[test_index]\n",
    "            y_train, y_test = species_train.values[train_index], species_train.values[test_index]\n",
    "            \n",
    "            # Now, I will fit my SVM model in the k-1 folds!\n",
    "            classif = SVC(C=C1, kernel='rbf', gamma=gamm, decision_function_shape='ovr')\n",
    "            classif.fit(X_train, y_train)\n",
    "            list_10fold.append(1 - classif.score(X_test, y_test))\n",
    "        # Calculate the average error of list_10fold and save it in ave_CV_errors list\n",
    "        ave_CV_errors.append(sum(list_10fold) / len(list_10fold))\n",
    "\n",
    "# Convert the ave_CV_errors list into a numpy array\n",
    "Arrray = np.asarray(ave_CV_errors)\n",
    "Arrray = Arrray.reshape(10,40) # Indexes are (C, gamma)\n",
    "# Take the indexes of the minimum element of the Arrray\n",
    "ind = np.unravel_index(np.argmin(Arrray, axis=None), Arrray.shape)\n",
    "\n",
    "# The list ave_CV_errors contains the average CV errors obtained by the 10 different values of C\n",
    "# and the 40 different values of gamma!\n",
    "# In order to pick the best values, we are going to pick the lowest value out of this list!!\n",
    "print(\"The minimum CV error happens for : C =\", C_range[ind[0]])\n",
    "print(\"The best gamma is gamma=\", gamma_range[ind[1]])\n",
    "print(\"The CV error for those values is: \", min(ave_CV_errors))\n",
    "\n",
    "# At this point, we have to train our model using the best values \n",
    "# calculated from the 10-fold cross validation!\n",
    "classif = SVC(C=C_range[ind[0]], kernel='rbf', gamma=gamma_range[ind[1]], decision_function_shape='ovr')\n",
    "classif.fit(features_train_augment_2, species_train)\n",
    "\n",
    "print(\"For the label 'Species', we have:\",\"\\n\")\n",
    "print(\"The accuracy of the model on the training set is: \", classif.score(features_train_augment_2, species_train)*100, \"%\")\n",
    "print(\"The cross-validation accuracy of the model is: \", (1 - min(ave_CV_errors))*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, using Classifier Chain method with SVM with Gaussian Kernel, we have: \n",
      "\n",
      "The Exact Match score is: 0.9911996294580825\n",
      "The Hamming Loss is: 0.00725644588544079\n"
     ]
    }
   ],
   "source": [
    "# Question b -> v (Continue)\n",
    "\n",
    "# Classifier for label 'Family'\n",
    "classif10 = SVC(C=1000, decision_function_shape='ovr', gamma=2.5, kernel='rbf')\n",
    "classif10.fit(features_train, family_train)\n",
    "y1_pred = classif10.predict(features_test)\n",
    "\n",
    "# Now, I have to augment my features test dataset\n",
    "# with the predicted \"Family\" label from the first classifier\n",
    "# and let my second classifier predict the label \"Genus\" by\n",
    "# providing him the augmented set of features.\n",
    "\n",
    "# Since I changed the name before, I have to do it also for the test set.\n",
    "le.fit(y1_pred)\n",
    "y1_pred = le.transform(y1_pred)\n",
    "        \n",
    "features_test1 = np.concatenate((features_test, np.expand_dims(y1_pred, axis=1)), axis=1)\n",
    "#Convert it into a dataframe\n",
    "features_test_augment_1 = pd.DataFrame(data=features_test1,index=None,columns=None)\n",
    "        \n",
    "\n",
    "# Classifier for label 'Genus'\n",
    "classif11 = SVC(C=1000000, decision_function_shape='ovr', gamma=0.9, kernel='rbf')\n",
    "classif11.fit(features_train_augment_1, genus_train)\n",
    "y2_pred = classif11.predict(features_test_augment_1)\n",
    "                                       \n",
    "# Now, I have to augment my already augmented features test dataset\n",
    "# with the predicted \"Genus\" label from the second classifier\n",
    "# and let my third classifier predict the label \"Species\" by\n",
    "# providing him the augmented set of features.\n",
    "\n",
    "# Since I changed the name before, I have to do it also for the test set.\n",
    "le.fit(y2_pred)\n",
    "y2_pred = le.transform(y2_pred)\n",
    "        \n",
    "features_test2 = np.concatenate((features_test1, np.expand_dims(y2_pred, axis=1)), axis=1)\n",
    "#Convert it into a dataframe\n",
    "features_test_augment_2 = pd.DataFrame(data=features_test2,index=None,columns=None)\n",
    "\n",
    "# Classifier for label 'Species'\n",
    "classif12 = SVC(C=100, decision_function_shape='ovr', gamma=0.5, kernel='rbf')\n",
    "classif12.fit(features_train_augment_2, species_train)\n",
    "\n",
    "# Now, I will concatenate the 3 predicted labels into 1 numpy array\n",
    "y_pred = np.concatenate((np.expand_dims(classif10.predict(features_test), axis=1), \n",
    "                         np.expand_dims(classif11.predict(features_test_augment_1), axis=1),\n",
    "                         np.expand_dims(classif12.predict(features_test_augment_2), axis=1)), axis=1)\n",
    "\n",
    "# Now, I will convert the true labels into a numpy array\n",
    "y_true = np.concatenate((np.expand_dims(np.asarray(family_test), axis=1), \n",
    "                         np.expand_dims(np.asarray(genus_test), axis=1),\n",
    "                         np.expand_dims(np.asarray(species_test), axis=1)), axis=1)\n",
    "\n",
    "# Calculate the Exact Match score and the Hamming Loss\n",
    "EMscore = 0\n",
    "count_Ham = 0\n",
    "for i in range(0,y_true.shape[0]):\n",
    "    count_EM = 0\n",
    "    for j in range(0,y_true.shape[1]):\n",
    "        if y_true[i][j]==y_pred[i][j]:\n",
    "            count_EM += 1\n",
    "        else:\n",
    "            count_Ham += 1\n",
    "    if count_EM==3:\n",
    "        EMscore += 1\n",
    "EMscore = EMscore / y_true.shape[0]\n",
    "Hamming_Loss = count_Ham / (y_true.shape[0] * y_true.shape[1])\n",
    "            \n",
    "print(\"So, using Classifier Chain method with SVM with Gaussian Kernel, we have:\",\"\\n\")\n",
    "print(\"The Exact Match score is:\", EMscore)\n",
    "print(\"The Hamming Loss is:\", Hamming_Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
